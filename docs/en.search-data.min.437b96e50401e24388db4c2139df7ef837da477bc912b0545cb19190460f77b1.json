[{"id":0,"href":"/posts/0000-Building-Docker-images-on-Travis.ci/","title":"Building Docker images on Travis.ci","section":"Posts","content":"Olá!\nInspired by the posts from Viktor Adam\u0026rsquo;s blog, when I have some spare time I have been playing with Docker and some open source tools to build this blog.\nSo far I have my two little devices a Raspberry Pi and a Rock64 hosting this blog, yes, that\u0026rsquo;s right, you\u0026rsquo;re reading this straight from a Ghost blog behind an NGinx hosted at my home, be nice and say hi!\n Appart of some details like sensible public data management and external CI usage to generate and publish Docker images that is the subject of this post, this is how my stack looks like right now. It will evolve soonish :]\n Sensible public data #  As we can see from the description of this repo of mine on Docker hub, some environment variables are expected as parameters to create containers. Some sensible data like Cloudflare API key goes to a public repository on Github, the .env file in binary form instead of row text.\nI could keep it somewhere on my local file system and use it to create containers but if I\u0026rsquo;ve learned something from past events is, maintaining files on my machine is the same of losing them sooner or later so, better to have them on the cloud where smarter people than me are taking care of backups.\nHowever, how to keep this kind of data safe in a public space? If you are using git a good option is to use Git Crypt.\nGit Crypt #  First, install it, following this guide.\nOnce you have installed it you need a key to lock and unlock the files you want, there are two options, the Symmetric Mode where you export a key and refer it to unlock your files and GPG Mode where you create a key and import the public key wherever you need to unlock files. I have decided to use Symmetric Mode, this time \u0026ldquo;after few disappointments with the GPG Mode\u0026rdquo; . The setup is easy and do not require further steps after you export your key.\nSetting up git-crypt and exporting a symmetric secret key #  Now on the git repository initiate, git-crypt like with a regular git init and export your symmetric key. To accomplish this you only need to perform the steps below.\ngit-crypt init git-crypt export-key /lab/security/git-cript-key  Now you need to create the .gitattributes at the root of your project and list the files you want to encrypt, the syntax is the same used by .gitignore files. Eg: *.env filter=git-crypt diff=git-crypt\nFrom this point .env will be automatically encrypted before being pushed to the repository. To unlock it in different machines, you need to run git-crypt unlock referring the secret key that you\u0026rsquo;ve just exported, in my case git-crypt unlock /lab/security/git-crypt-key\n If you have the intention to unlock this files somewhere else, you need to have access to this key there too, the process is merely the same, unlock the files with the same command above.\n Setting up a Trevis.ci account #  At this point, you probably have something like that on your GitHub. The next step is to build your images and publish them automatically when you update your repository.\nUsing your GitHub account, Sign Up to Trevis.ci and enable the integration for this repo.\nSetting up the .travis.yml file #  To tell Travis what to do, you need a .travis file on your repository, you can learn about it here to create one that works for you. Mine looks like that:\nsudo: required services: - docker script: # setup QEMU - docker run --rm --privileged multiarch/qemu-user-static:register --reset # build images - docker build -t ddns:$DOCKER_TAG -f Dockerfile.$DOCKER_ARCH . # push images - docker login -u=\"$DOCKER_HUB_LOGIN\" -p=\"$DOCKER_HUB_PASSWORD\" - docker tag ddns:$DOCKER_TAG allandequeiroz/cloudflare-ddns:$DOCKER_TAG - docker push allandequeiroz/cloudflare-ddns:$DOCKER_TAG -  if [ \"$LATEST\" == \"true\" ]; then docker tag ddns:$DOCKER_TAG allandequeiroz/cloudflare-ddns:latest docker push allandequeiroz/cloudflare-ddns:latest fi env: matrix: - DOCKER_TAG=arm DOCKER_ARCH=arm LATEST=true - DOCKER_TAG=amd64 DOCKER_ARCH=amd64  Since I\u0026rsquo;m using Raspberry Pi and Rock64, I need to build images for arm architecture, and since I want to be nice to people, I\u0026rsquo;m generating amd ones even that they are useless for me.\nYou can ask Travis to do that defining the matrix section, this way the script part will be executed once for each line of the matrix, in this case, every time I push something new to GitHub, two new images will be generated and pushed to Docker hub.\nAnother interesting detail here is the ability to define environment variables, as you may notice my Docker hub credentials to push images: docker login -u=\u0026quot;$DOCKER_HUB_LOGIN\u0026quot; -p=\u0026quot;$DOCKER_HUB_PASSWORD\u0026quot;\nTravis provides a settings interface where you can define environment variables and use them instead of keeping it public on your repository.\nOk. Now you need to push something new to your repository or Trigger the build from More options on Travis interface, by the end you\u0026rsquo;ll see something like that on Travis, and your images will be available on Docker hub.\nTravis badges #  After all this effort you deserve some badges, you can get one straight from your Travis account, just ask for the .svg file related to your repository. Travis update it after each build so take care to not break the build :)\nhttps://api.travis-ci.org/allandequeiroz/cloudflare-ddns.svg\nMicrobadger badges #  To have some badges from your Docker hub account you can use Microbadger, once again Sign Up using your GitHub account and link it to your Docker Hub account, after few seconds you\u0026rsquo;ll have some badges waiting for you, in this case, version the architecture and layers amount of layers of your image.\nhttps://images.microbadger.com/badges/version/allandequeiroz/cloudflare-ddns.svg https://images.microbadger.com/badges/image/allandequeiroz/cloudflare-ddns.svg Showing off your badges #  In the end, you can use something like the lines below to display your badges on your README.md files anywhere.\n[![Travis](https://api.travis-ci.org/allandequeiroz/cloudflare-ddns.svg)](https://travis-ci.org/allandequeiroz/cloudflare-ddns) [![](https://images.microbadger.com/badges/version/allandequeiroz/cloudflare-ddns.svg)](https://microbadger.com/images/allandequeiroz/cloudflare-ddns \"Get your own version badge on microbadger.com\") [![](https://images.microbadger.com/badges/image/allandequeiroz/cloudflare-ddns.svg)](https://microbadger.com/images/allandequeiroz/cloudflare-ddns \"Get your own image badge on microbadger.com\") [![](https://images.microbadger.com/badges/version/allandequeiroz/cloudflare-ddns:amd64.svg)](https://microbadger.com/images/allandequeiroz/cloudflare-ddns:amd64 \"Get your own version badge on microbadger.com\") [![](https://images.microbadger.com/badges/image/allandequeiroz/cloudflare-ddns:amd64.svg)](https://microbadger.com/images/allandequeiroz/cloudflare-ddns:amd64 \"Get your own image badge on microbadger.com\")       \n"},{"id":1,"href":"/posts/0003-BuildingBlocks/","title":"BuildingBlocks","section":"Posts","content":"Olá!\nToday I would like to write a bit about Java Collections. I believe most of us that already played with Java know about the primary and most used classes in the Java Collections framework, at least ones that ever had an interview are familiar with, they are part of the default pack of questions that interviewers ask to check if you at least read about Java.\nBut this post does not intend to talk about the different aspects of data structures, or when to use A or B, the idea is to speak briefly about collections in a concurrent context.\nThere are some aspects of Java that were real reasons for concern in the past and still causing skepticism nowadays, example EJBs, reflection or concurrency in general. Who does not have that strange feeling when someone mention Vector, HashTable or StringBuffer? And who do not transfer those mixed feelings to Concurrent\u0026lt;Something\u0026gt;Set,Map,Queue,Deque... for example?\nSomething got lost in the way about concurrency. Vector, HashTable and StringBuffer are synchronized not concurrent. There are differences.\nRegular collections #  I\u0026rsquo;ll assume that we don\u0026rsquo;t need to spend much time over here, you probably have been playing with HashSet, ArrayList, LinkedList and HashMap at some point. These classes are just great and probably another way to verify Pareto\u0026rsquo;s principle in the Java context.\nSo if you know you\u0026rsquo;re not sharing your collection at any moment, use them and be happy. By sharing, I mean something around this.\nSynchronized collections #  Now we got to the point that is probably one of the causes behind all the mysticism around race condition control slowness, contention. But what could cause contention to classes such as Vector or HashTable? To answer that let\u0026rsquo;s take a look into Collections.synchronizedList and see what happen when we use synchronized collections.\n// The first step is a simple check to verify if the provided List // is an instance of RandomAccess and decide who is wrapping it. public static List synchronizedList(List list) { return (list instanceof RandomAccess ? new SynchronizedRandomAccessList(list) : new SynchronizedList(list)); } // Now the SynchronizedList constructor receives our List and keeps // an internal reference to it, just notice that SynchronizedList is // passing this list to it's parent SynchronizedCollection, another // static inner class inside Collections SynchronizedList(List list) { super(list); this.list = list; } // This is how SynchronizedCollection's constructor looks like, once // again it keeps an internal reference, but the important part is the // mutex that will be used to synchronize almost all methods around the // original List. SynchronizedCollection(Collection c) { this.c = Objects.requireNonNull(c); mutex = this; } // What's the mutex for public boolean add (E e){ synchronized (mutex) { return c.add(e); } } public E get (int index){ synchronized (mutex) { return list.get(index); } } public boolean removeAll (Collection coll){ synchronized (mutex) { return c.removeAll(coll); } }  As you can see, except for spliterators and streams related methods, all the other ones are synchronized this way, blocking entire methods from outside using the same object to lock everything, this is thread safe but when a thread is holding this lock, doesn\u0026rsquo;t matter what\u0026rsquo;s going on, no one else will perform anything over this Collection. And the mechanism is similar for the other synchronized collections too. If you use one of the methods below, this is how you\u0026rsquo;re protecting your collection against race conditions.\nCollections.synchronizedList(List list) Collections.synchronizedCollection(Collection c) Collections.synchronizedMap(Map m) Collections.synchronizedNavigableMap(NavigableMap m) Collections.synchronizedNavigableSet(NavigableSet s) Collections.synchronizedSet(Set s) Collections.synchronizedSortedMap(SortedMap m) Collections.synchronizedSortedSet(SortedSet s)  But that\u0026rsquo;s all right. It doesn\u0026rsquo;t mean you\u0026rsquo;ll have an awful performance just because you\u0026rsquo;re using it. Then when should we use a synchronized collection?\n If the collection is shared, but access is not too frequent, we\u0026rsquo;re safe to use it.  Just remember if you\u0026rsquo;re using large collections it won\u0026rsquo;t scale that well and even using a synchronized collection still necessary to provide additional locking to guard compound actions, e.g.:\n Iteration Navigation Conditional operations - putIfAbsent  Concurrent collections #  Concurrent collections are specific versions designed with concurrency in mind by design, instead of using a single collection-wide lock it uses the concept of segmentation supported by (internal data structures).\nLet\u0026rsquo;s use ConcurrentHashMap as an example, segments (see upfront) will be locked just during updating operations and even during these operations, synchronization will happen in specific moments, is almost surgical. Take a look for example into ConcurrentHashMap.putVal.\nOne of ConcurrentHashMap's inner classes is Segment kept in a bucket table that holds chunks of this HashMap, this way different threads can operate in different segments reducing contention.\nSegment\u0026#x3C;K,V\u0026#x3E;[] segments = (Segment\u0026#x3C;K,V\u0026#x3E;[]) new Segment\u0026#x3C;?,?\u0026#x3E;[DEFAULT_CONCURRENCY_LEVEL];  DEFAULT_CONCURRENCY_LEVEL is the \u0026ldquo;expected\u0026rdquo; number of concurrently updating threads operating over this Map, by default setted to 16 meaning that during high concurrency moments at least 16 threads can operate at the same time over this Map (We\u0026rsquo;ll have 16 locks, each one guarding each segment or bucket if you prefer).\nIf you take a look at the ConcurrentHashMap documentation you\u0026rsquo;ll see that we can change the default values to something more realistic for the scenario in hand, apart of concurrencyLevel is possible to specify loadFactor that tells how much the map should grow in case it has to, by default the growth factor is 0.75%. The initialCapacity, if we have an idea of how big will be the Map, we can avoid the resizing overhead using this parameter, the default again is 16.\nAt this point, we know that internally the ConcurrentHashMap breaks it\u0026rsquo;s content in segments so threads can work in different parts of it simultaneously without interference.\nAll right then, but how does it know where to go which segment when reading or writing content after breaking it up?\nBased on the key hash calc int hash = spread(key.hashCode()) the bucket is identified, created or resized than the new Node\u0026lt;k,v\u0026gt; is inserted. At this point the insertion, we finally have a synchronized block.\nWith this notion about the internal complexity difference between a synchronized and a concurrent collection, you may are wondering when to use the concurrent option.\nThe answer is if the collection will be shared frequently and accessed by multiple threads, for sure a concurrent collection would be handy. Just remember, it might use more memory, especially ConcurrentHashMap why? To support all this mechanism, each Segment is a ReentrantLock, internally ReentrantLock maintain an inner class called Sync that extends AbstractQueuedSynchronizer that holds a queue of nodes to maintain the state of the threads, ending up in few more memory usage. At this point, you can see things such as.\n/** * The thread that enqueued this node. Initialized on * construction and nulled out after use. */ volatile Thread thread; volatile int waitStatus; So on...  Ok, I got it! I won\u0026rsquo;t use it otherwise memory will blow up!!\nNo. Not necessarily, I\u0026rsquo;ve performed some silly tests between HashMap, synchronized HashMap, and ConcurrentHashMap to see the difference.\nThe first round 1_000_000 writes and reads with a single Thread.\n# HashMap Time = 3.8s totalMemoryAllocated=538.500MB # Synchronized HashMap Time = 3.9s totalMemoryAllocated=540.500MB # Concurrent HashMap Time = 5.5s totalMemoryAllocated=541.500MB The second round 1_000_000 writes and reads against four threads.\n# HashMap Time = 4.1s totalMemoryAllocated=538.500MB # Synchronized HashMap Time = 3.6s totalMemoryAllocated=539.500MB # Concurrent HashMap Time = 4.5s totalMemoryAllocated=540.000MB As you can see, not bad at all so unless you\u0026rsquo;re fighting for each nanosecond you better think first about preventing wrong results and/or race condition problems.\nBonus #  If you\u0026rsquo;re not happy with the alternatives provided by the Java language, there are options out there, some already well known, e.g., Google Guava or more specific one\u0026rsquo;s example:\n Eclipse Collections\n Eclipse Collections is the best Java collections framework ever that brings happiness to your Java development.\n  Minimize object creation Work with large data sets Support mutable and immutable collections   JCTools\nSupports various specialized queues, such as\n SPSC - Single Producer Single Consumer (Wait-Free, bounded and unbounded) MPSC - Multi-Producer Single Consumer (Lockless, bounded and unbounded) SPMC - Single Producer Multi-Consumer (Lockless, bounded) MPMC - Multi-Producer Multi-Consumer (Lockless, bounded)   Cliff Click\u0026rsquo;s High Scale Library\n A collection of Concurrent and Highly Scalable Utilities. These are intended as direct replacements for the java.util.* or java.util.concurrent.* collections but with better performance when many CPUs are using the collection concurrently. Single-threaded performance may be slightly lower.\n Conclusion #  That fear of certain Java mechanisms performance are gone or at least minimized, today they are real alternatives to solve practical problems and knowing them is important to avoid coding mistakes or debates using arguments from 10+ years ago.\nKnowing a bit more about the nuances of the available options is useful not to banish a tool but to choose it correctly at the proper moment.\nReferences #   ConcurrentHashMap\n CopyOnWriteArrayList\n CopyOnWriteArraySet\n ConcurrentHashMap.KeySetView\n ConcurrentLinkedDeque.html\n ConcurrentLinkedQueue.html\n ConcurrentMap.html\n ConcurrentNavigableMap.html\n ConcurrentSkipListMap.html\n ConcurrentSkipListSet.html\n"},{"id":2,"href":"/posts/hugoisforlovers/","title":"Getting Started with Hugo","section":"Posts","content":"Step 1. Install Hugo #  Go to Hugo releases and download the appropriate version for your OS and architecture.\nSave it somewhere specific as we will be using it in the next step.\nMore complete instructions are available at Install Hugo\nStep 2. Build the Docs #  Hugo has its own example site which happens to also be the documentation site you are reading right now.\nFollow the following steps:\n Clone the Hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313  Corresponding pseudo commands:\ngit clone https://github.com/spf13/hugo cd hugo /path/to/where/you/installed/hugo server --source=./docs \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Press ctrl+c to stop  Once you\u0026rsquo;ve gotten here, follow along the rest of this page on your local build.\nStep 3. Change the docs site #  Stop the Hugo process by hitting Ctrl+C.\nNow we are going to run hugo again, but this time with hugo in watch mode.\n/path/to/hugo/from/step/1/hugo server --source=./docs --watch \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Watching for changes in /Users/spf13/Code/hugo/docs/content \u0026gt; Press ctrl+c to stop  Open your favorite editor and change one of the source content pages. How about changing this very file to fix the typo. How about changing this very file to fix the typo.\nContent files are found in docs/content/. Unless otherwise specified, files are located at the same relative location as the url, in our case docs/content/overview/quickstart.md.\nChange and save this file.. Notice what happened in your terminal.\n\u0026gt; Change detected, rebuilding site \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 26 ms  Refresh the browser and observe that the typo is now fixed.\nNotice how quick that was. Try to refresh the site before it\u0026rsquo;s finished building. I double dare you. Having nearly instant feedback enables you to have your creativity flow without waiting for long builds.\nStep 4. Have fun #  The best way to learn something is to play with it.\n"},{"id":3,"href":"/posts/0006-Home-made-Kubernetes-cluster/","title":"Home made Kubernetes cluster","section":"Posts","content":"Homemade Kubernetes Cluster #  In my previous post, I wrote about how to set up a scalable Jenkins using Kubernetes at home on top of home devices, in my case, 3 Raspberries, 2 Rock64 and a NUC. Since it wasn\u0026rsquo;t a 5 minutes task I\u0026rsquo;ll describe here the steps I went through to get it working.\nPre-checks #  Before installing Kubernetes you better check which Docker version is supported by the Kubernetes version you intend to install, a few days ago I upgraded my Kubernetes cluster from version 12 to 15 so, version 15 is what I\u0026rsquo;ll be used as a reference here.\nIf you go to the Kubernetes Changelogs page and look for Docker you\u0026rsquo;ll spot the \u0026ldquo;validated docker versions\u0026rdquo;, as we can see, I could have upgraded to the version 18.09, but in the end, I\u0026rsquo;ve decided by 18.6.2.\nSaid that before installing Docker and Kubernetes it might be a good idea to upgrade your whole system if it is safe to do so.\n Docker #  Once you determined which Docker version to install, before moving forward you need to keep a few details in mind such as the device architectures you also have the OS version. You may need to edit these details at your \u001dsources.list\u001d otherwise the downloaded packages won\u0026rsquo;t work.\n If you have any device which the architecture is not amd64, remove [arch=amd64] from sources.list. Instead of $(lsb_release -cs) you can specify the OS release you want to target by hand if you want, eg: xenial. Package version may look strange but it\u0026rsquo;s correct, eg: docker-ce=18.06.2~ce~3-0~ubuntu\u001c.  Once you\u0026rsquo;re done with the steps above you can add the repository keys so you won\u0026rsquo;t get any harmful package by accident.\n Now is time to include the sources list per se, please notice the two options below, the first one is for anything that is not ARM, the second one for ARM architectures.\u001c\nAnything but ARM\n ARM\n Install Docker\n Setup Docker daemon\n Restart Docker\n At this point, we should have a Docker version fully compliant with Kubernetes, up and running.\nDocker Compose #  Since we\u0026rsquo;re installing Docker, just in case you decide to spin up a docker-compose for some reason, let\u0026rsquo;s install it as well. The points of attention are similar to the one used during the Docker installation.\u001c\nAnything but ARM\n ARM\nThe process to have docker-compose at the ARM platform was a bit more complicated, at least for me, but is not so complicated, here are the steps I went through:\u001c\u001c\n Docker Compose bash autocompletion #  Now that you have docker-compose everywhere is time to set the autocompletion to spare a bit of your patience down the road with typing.\u001c\n Kubernetes #  With a fully happy Docker running across all the devices, it is time to install Kubernetes.\n Initialising Master Kubernetes #  This step will be easier if you save the script below at your master host, execute, and let it do its job. You can run step-by-step by hand if you want, just like the ones before, this is up to you.\n By the end you\u0026rsquo;ll see amongst the logged lines, one starting with kubeadm join, copy this whole line and execute across the other devices you have. The line I\u0026rsquo;m talking about is similar to this:\u001c\nkubeadm join 192.168.1.2:6443 --token zd07uq.d91cxeg22nhwl6ti --discovery-token-ca-cert-hash sha256:551f848676f99621bbed06810d15532f69019398d18d475a0cbaa9e69ee9d195\nAt this point you should have all your devices communicating with the manager, to check how is it going execute kubectl get nodes -o wide at the manager node, maybe the nodes are not ready yet, give it some time, they need to perform some tasks as well. Once you see something like the lines below, you\u0026rsquo;re ready to go.\n Kubernetes autocompletion #  Last but not least, kubectl completion, believe me, this one is a must-have, especially about the Pod\u0026rsquo;s names, you won\u0026rsquo;t want to copy and paste or even worse, type the names Kubernetes gives to the Pods.\necho \u0026ldquo;source \u0026lt;(kubectl completion bash)\u0026rdquo; \u0026raquo; ~/.bashrc\nConclusion #  It was a short step-by-step on how to set up your Kubernetes cluster; I believe it\u0026rsquo;ll be useful, especially if you have devices with different architectures. It may not be fully compatible with the scenario, but for sure it\u0026rsquo;ll give you some ideas to solve your problems.\nAnother thing, if you\u0026rsquo;re looking to go beyond and have your bare metal Load balanced cluster, take a look at MetalLb. And do not forget to take a look at the Cluster Networking page as well, the kubernetes_master.sh installed Wave Net for you but, you may want a different one.\nHope it helps!\n"},{"id":4,"href":"/posts/0008-Jackson-field-converters/","title":"Jackson field converters","section":"Posts","content":"Custom serialisation with Jackson at field level #  Quick one to not forget since is quite useful. How to specify custom serializers at field level.\nThe example here is a bit ridiculous but a few days ago I needed it for a real scenario and was quite hand so, ignore the example since I couldn\u0026rsquo;t think about anything better and keep your focus on how handy it may be.\nModel #   Serializer #   Deserializer #   Test #   Result #   "},{"id":5,"href":"/posts/0007-Jenkins+Kubernetes/","title":"Jenkins+Kubernetes","section":"Posts","content":"How to Setup Scalable Jenkins on Top of a Kubernetes Cluster (At Home). #   Before we start, I\u0026rsquo;ll make some intro about the scenario and reasons for my choices, if you prefer, skip this part, the meat of the article starts at the Docker section.\n A few days ago I decided to start a new side project, a tool that will take care of a bunch of things I need; I pay for different ones, and yet they don\u0026rsquo;t cover everything I need. Since I have another excuse, and, it\u0026rsquo;s for myself, I\u0026rsquo;ve decided to go fully buzzword-compliant, something hard to achieve at work.\nOne of the critical points is the building process of course, I could use Travis like I do some times but since the source code is private, and I have no intention to pay for Travis I\u0026rsquo;ve decided to spin up a Jenkins running on top of my beloved home devices, later it may change, but for now it\u0026rsquo;ll be like that.\nJust as a matter of fact, these are the devices I\u0026rsquo;m talking about.\n Another thing that worth to mention, the version I\u0026rsquo;ll describe here is the first version I got working, the simplest one, accessing the master Jenkins through NodePort. The final version was a bit longer road and would make this article far longer it has two services, one for the jnlp of type ClusterIP and another one, the http of type LoadBalancer.\nThe problem with the final version is that in my case \u0026ldquo;I\u0026rsquo;m not sure if all this was really necessary\u0026rdquo; I had to use Metallb to have my internal bare metal Load Balancer also because of the BGP configuration, I had to flash my Netgear and replace its original firmware by dd-wrt and also, a few other configurations here and there.\nIf was necessary or not, at least it was quite fun.\nAssumptions #  I\u0026rsquo;ll assume that if you\u0026rsquo;re here reading about Jenkins and Kubernetes is because you already have some knowledge about any of the container runtime environments supported by Kubernetes, since I know only about Docker, I\u0026rsquo;ll stick with it.\nI\u0026rsquo;ll assume as well that you already have your Kubernetes cluster up and running, with all the nodes ready to go, running a kubectl get nodes -o wide you should get enough information.\n Docker #  To start with, let\u0026rsquo;s take a look at the Dockerfile, the idea here is to create a basic image ready to go with the initial necessary plugins. By ready to go I mean the latest version, interesting to know that the latest Jenkins image at Docker Hub doesn\u0026rsquo;t contain the latest Jenkins version, if you tried to setup Jenkins before, you know what it means, many plugins will refuse work with an outdated Jenkins version.\nLines 3 to 5 (see below) takes care of the platform update so the plugins will be happy when Jenkins starts. Luckily updates.jenkins-ci.org keeps a link to the latest bundle, so we need to point there and problem solved.\n To build this image you need to execute the following commands, please replace allandequeiroz/jenkins by \u0026lt;your user at docker hub\u0026gt;/\u0026lt;the name you prefer\u0026gt;.\n The commands above will build and push the latest version. Alternatively, you can set a specific version as well.\n Kubernetes #  As mentioned before, for the sake of simplicity, I\u0026rsquo;ll place here my initial working version, without load balancing capabilities, for this version you\u0026rsquo;ll access your Jenkins instance at port 30000 of a fixed host, defined by a tag.\n If you prefer, you can see an alternative version here. If what you need is more likely the alternative version, please read the first part of the article, before \u0026ldquo;Assumptions\u0026rdquo;, because you may need some more steps if you\u0026rsquo;re deploying it at home.\n  A bit of explanation about this file:\nLines: 16-17 - We\u0026rsquo;re telling Jenkins to skip the initial wizard so we won\u0026rsquo;t need to install any other plugin, set any password or get any hash from the logs right now when it starts, we\u0026rsquo;ll go straight to business.\nLines: 26-28 - We\u0026rsquo;re mounting a volume at the master instance to keep our configurations alive across different deployments.\nLines: 31-32 - These are the lines that define where Kubernetes will place your Jenkins master instance.\nSince you have nodeSelector, before you execute this file, you need to pick one of your nodes to host the Jenkins master, to do so, execute the lines below. Please replace boss by the name of your node.\n The first line adds the label; the second one will give you a description about the node, look for Labels: you should see amongst the labels a line like jenkins=master.\nAssuming that you\u0026rsquo;ve saved the yaml above in a file called jenkins.yaml, is time to apply it; to do so, you must be at your master host.\n This process sometimes turns to be a bit repetitive and tedious, if you prefer, use a shell script to make it a bit easier, here you\u0026rsquo;ll find an example.\n  If you\u0026rsquo;ve executed this script you\u0026rsquo;ll see an output similar to the one below, if not, copy and paste the first lines after the echoes, and I\u0026rsquo;ll manage to see what you have so far.\n Notice that the service doesn\u0026rsquo;t have any information about its public address, but we\u0026rsquo;ve specified a host where Jenkins should be deployed the same one you labelled before, with this in hand and the nodePort we\u0026rsquo;ve specified we have enough information to access Jenkins. In my case http://boss:30000\nJenkins #  Before jumping into the Kubernetes plugin configuration, let\u0026rsquo;s take care of something essential, credentials. First, let\u0026rsquo;s create a service account and fetch the secret.\n Copy the whole content printed at the console by the third command and go to Jenkins \u0026gt; Credentials \u0026gt; System \u0026gt; Global credentials \u0026gt; Add Credentials, change the Kind drop-down options to Secret text and past into Secret, set an ID to make it easy to pick it up later. There are many ways to handle credentials, but I\u0026rsquo;ve picked this approach because I think it would be easier to set up different environments later.\nNow let\u0026rsquo;s create our configuration at Jenkins \u0026gt; Manage Jenkins \u0026gt; Configure System \u0026gt; Add a new cloud \u0026gt; Kubernetes\nNow, we\u0026rsquo;ll need a few pieces of information to set up our Jenkins cluster:\n Kubernetes URL: At the master node executes kubectl cluster-info | grep master and copy the URL you get there, for example, https://192.168.1.101:6443. Credentials: Select the credentials we\u0026rsquo;ve created at the previous step. Jenkins tunnel: This information is critical to avoid falling into problems such as connection refused or jnlp is not enabled, you\u0026rsquo;ll need to point to your service endpoint, do you remember the name of the service we\u0026rsquo;ve provided at the jenkins.yaml? In this case it was jenkins-master-svc if you changed, get back there and double-check, we\u0026rsquo;ll need it to build the service FQDN \u0026lt;service name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local:\u0026lt;port\u0026gt;, in this case, jenkins-master-svc.default.svc.cluster.local:50000.  Anything else you can copy from the screenshots below at this time, after that, play around with the configurations until you get what you need.\n  Ok, now let\u0026rsquo;s create some dummy build projects, a Freestyle project will do, at Build \u0026gt; Execute shell add something silly to start with, for example sleep 30, this job will take a while to finish, and we manage to see multiple instances working in parallel. Create two of three to see it working for real.\n Back to the main screen start building all of them and once you see new entries at Build Executor Status execute the following command to see their real status and the node where they are running at.\n By the end, you should see the sunny Jenkins interface.\n From this point you have your Jenkins cluster up and running, is up to you know to play with the many configuration options provided by the Kubernetes plugin and to set up your build projects.\n"},{"id":6,"href":"/posts/0005-Terraform/","title":"Terraform","section":"Posts","content":"TLDR; A walk through an EC2 instance setup, dynamically attaching volumes snapshots and configuring Cloudflare DNS entries pointing to the new instance using Terraform and Docker (Ghost, Traefik).\nSource code: https://github.com/allandequeiroz/blog/tree/extract_variables\nBasic infrastructure configuration using Terraform, Docker (Ghost, Traefik) and Cloudflare. #  For a while, I\u0026rsquo;ve hosted this blog, at home, using Docker and Kubernetes across a few Raspberries (I use it as an excuse to do play around). It turns out that I\u0026rsquo;m moving to a new place, so I have a new excuse to play with something else.\nThe idea at this time is simply to provide the infrastructure on EC2 using Terraform, set up the DNS entries and keep the blog alive, later on, I\u0026rsquo;ll see what can I do with Kubernetes and perhaps include some load-balancing and autoscaling as well (even completely unnecessary due the low traffic) but for now let\u0026rsquo;s take a look at the current configurations:\n Docker (docker-compose)  Ghost Traefik   Traefik (toml) Terraform  EC2 Cloudflare    Docker #  Ghost and Traefik #  Different from my previous setup, at this time I\u0026rsquo;ve decided to move out from my customized Ghost version and start using a vanilla one provided by Ghost\u0026rsquo;s maintainers available on Docker hub, the only thing to keep was the config.production.json file to set details such as port, database and content placement; if you want to check this file, it\u0026rsquo;s available at the repository.\nAlso, to play with something new, I\u0026rsquo;ve replaced NGINX by Traefik; it was quite interesting to know that Traefik deals with all the underline details also is quite dynamic in terms of self-reconfiguration, we give it a .toml file and the tricks start to happen; we\u0026rsquo;ll see a bit more later on, for now, let\u0026rsquo;s have a look at the docker-compose file.\n As you can see, there are not so many details; we\u0026rsquo;re basically specifying two services, a network and setting up some labels to help Traefik does its job. The volumes mappings are mostly optional but, here we\u0026rsquo;re placing some existing configuration and persisting data outside of the containers, at the host file system, no strict rules here as well, in this case for example, since I\u0026rsquo;ve told Ghost that the content path is /var/lib/ghost/content a volume was mounted at this same location so the whole data will be persisted at the host FS, the volumes' snapshots will have the same content, so when new EC2 instances are launched, data from previous instances will be present.\nThe labels as mentioned before are to help Traefik but for now just notice the mapping to docker.sock, if you\u0026rsquo;re not familiar with its usage, is basically the Unix socket that the Docker daemon listens to so containers can communicate with it from within, basically containers are able to consume the API through the socket. Traefik, in this case, observe the Docker events through the API and depending on the events, decides what to do about the current configuration, if something needs to be destroyed, created, recreated, changed and so on.\nTraefik #  Traefik is a very clever reverse proxy; it deals by itself with many complexities for a low price in terms of configuration as you can see below.\n From this short configuration we have support to HTTP/HTTPS, ports configuration, the whole boilerplate to generate our certificates with Let\u0026rsquo;s Encrypt and a dashboard.\n Concerning the dashboard, worth to mention that using the [api] section exposes Traefik\u0026rsquo;s configuration so remember to secure it with some authentication/authorization mechanism, in this example basic auth was used, when hitting the dashboard URL a pop up asking for credentials will be prompted. The password is generated by htpasswd for example echo $(htpasswd -nb \u0026lt;USER\u0026gt; password) | sed -e s/\\\\$/\\\\$\\\\$/g\n Terraform #  Now let\u0026rsquo;s have a look at the Terraform infrastructure definition for both EC2 and Cloudflare, I think a few things should be mentioned.\n First, there many ways to define and load variables with Terraform but the ones at this example are being loaded from the environment, when Terraform is executed it\u0026rsquo;ll start the lookup, the environment variables prefixed with TF_VAR will be considered by Terraform. As an example, at the .tf file we have the variable aws_access_key_id, the defined environment variable was TF_VAR_aws_access_key_id, for Terraform it\u0026rsquo;s a match. Second, we can break down the configurations as much as we want, just keep in mind that the files are loaded in alphabetical order, in this case it worked fine since Terraform will start from __a__ws.tf then proceed with __c__loudflare.tf. A necessity here since to define the DNS entries the EC2 instance IP address should be defined already. Third, the remote-exec is not the most optimal approach, would be possible, for example, to have AMIs ready to go with everything necessary installed and configured beforehand.  EC2 #  As said before, you can have a look at the full files at the repository, but here, I\u0026rsquo;ll break down in sections to give a short explanation for each one.\n Variables definitions, to make our configuration flexible/reusable also to hide sensitive information. Virtual Private Cloud (VPC) since a VPC is not like a regular datacenter with networks, switches, routers and so on but, instead, software-defined, we need(optional in fact) to define a private(isolated) section of the Cloud to launch our instances. VPC Subnet once defined our private Cloud; we need to specify our subnet to provide the behaviours we want, in this case, the subnet is associated with an Internet Gateway, turning this particular subnet into a public one so we can access this particular network space from the external world. Internet Gateway as mentioned above, the intention here is to make a particular subnet accessible from the internet, the gateway will work with the subnet to make it happen. Route Table definition into the VPC to route traffic to the Internet Gateway. Route Table Association to \u0026ldquo;link\u0026rdquo; a Subnet with a Route Table. Security Group to be used inside the VPC (this example shouldn\u0026rsquo;t be used in production, it\u0026rsquo;s exceptionally permissive). Amazon Machine Image (AMI) to specify details such as a particular image to be used to start new instances or to attaching a previously taken volume snapshot for example. Key Pair to provide access to the instance over SSH. EC2 Instance a description of how a new instance should look like, where and how it should be placed, for example, into which subnet, the Security Group to be used, Key Pair, which Volume Snapshot to be also used to execute commands over SSH. Many thinks are possible here; the previous description is about the context of this example.  With this in mind, let\u0026rsquo;s have a look at each of these sections.\nVariables\n As mentioned before, Terraform allows us to define variables so we can make our configurations more flexible also keep sensitive information out of sight, one interesting detail is that we can define default values, for example, we could have something like\n VPC\n The intention of defining a VPC is to have complete control but is not, in fact, a necessity, in case we skip this definition, a default one will be provided by AWS, but of course, AWS won\u0026rsquo;t guess what we intend.\nVPC Subnet\n The subnet definition is also important, in this case, there\u0026rsquo;s nothing fancy going on, but we could use it to create different subnets some to be exposed and some to be completely private.\nInternet Gateway\n To achieve the goal of routing internet traffic into the VPC, we need to specify an Internet Gateway.\nRoute Table\n The route table is what we use in association with the gateway to exposing a particular subnet to the internet.\nRoute Table Association\n This small section is the glue between a subnet and a route table; we use it to put them together.\nSecurity Group\n By default, AWS allows all traffic to go outside but terraform doesn\u0026rsquo;t so we need to state this definition explicitly, in this case, in and out.\nAMI\n This section is important to define how the instances should \u0026ldquo;look like\u0026rdquo;, in this case, as you can see, we have three sections.\n data: aws_ebs_snapshot to lookup for a specific volume snapshot to be mounted when creating new instances. data: aws_ami to lookup for a specific AMI to be used as a base for the new instance. resource: aws_ami to state the definitions of how the instances should be created, you can see that we\u0026rsquo;ve made use of both filterings we\u0026rsquo;ve defined above, one to set the block device and another to set which particular AMI to be used when instantiating.  The snapshots are taken automatically, the periodicity and target were defined beforehand through the Elastic Block Store Lifecycle Manager.\nKey Pair\n Here is the definition of which Key Pair to be associated with the new instances, this way if in possession of the .pem file, access to the instance over SSH is possible.\nEC2 Instance\n Ok, here is where we put everything together that\u0026rsquo;s why this section is slightly bigger, but if you read it line by line, you\u0026rsquo;ll notice that now we\u0026rsquo;re only setting values using what was defined before so you can see that there are only three sections.\n Values definitions the first seven lines where the values are coming from what was configured before. Connection configuration lines from nine to fourteen are describing how the connectivity should work. Commands the remaining lines (except for the tag definition) are the commands to be executed while spinning up a new instance.  The interesting thing here is about remote-exec because Docker is being installed and its service needs to be started; a new connection should happen, permissions also won\u0026rsquo;t work correctly if the connection is not closed, only from a new session the remaining commands will work appropriately.\nAnother detail to is the usage of tags, once the infrastructure is created, these tags will be present on each of these parts, they\u0026rsquo;re handy for organization and filtering purposes, at the end of the day I was feeling like tagging here was so important and useful as if I was doing something on Kubernetes for example.\nCloudflare #   A record defining the target domain, associated with the new instance IP address (we could be pointing to an LB for example). Canonical Name record (CNAME) to provide www. subdomain. Canonical Name record (CNAME) to provide access to Traefik dashboard, do you remember the labels at the docker-compose.yml? We\u0026rsquo;ve defined two traefik.frontend.rule entries, one for Ghost and another to Traefik so, given the requested URL, Traefik will redirect the request to the correct container.   The Cloudflare configuration is not extensive so let\u0026rsquo;s keep it as a single piece, there\u0026rsquo;s nothing mythical here but one line that worth to mention is the line 12. Here the IP associated with the new instance is taken and set to the new DNS entry, at first I got some errors at this part because the IP wasn\u0026rsquo;t yet defined, the line cloud-init status --wait at the end of the aws_instance creation tackles this problem, it will make Terraform wait until AWS finishes its job before carrying on with Cloudflare.\nConclusion #  After playing with this, apart of the usual excitement, it feels like we need something more powerful to work with configurations, I can imagine the number of files and probably untrackable repetitions for more complex configurations, fingers crossed that we have something better any time soon, maybe Dhall will be better and yet safe option, who knows.\nSo even with the current configuration scenario is quite fun to go through all this and see a whole infrastructure being created/destroyed in a matter of seconds thanks to the automation capabilities, tools and options available we have these days.\n"},{"id":7,"href":"/posts/0002-ThreadSafety/","title":"ThreadSafety","section":"Posts","content":"Olá!\nA few days ago I ended up into this StackOverflow thread from almost seven years ago, this guy was getting inconsistent results from a multithread code. Well, looked like an excellent excuse to recall some topics and do some new research.\nTo start with, let\u0026rsquo;s just quickly remember the differences between Multiprogramming, Multiprocessing, Multitasking, and Multithreading.\nMultiprogramming #  In a multiprogramming environment more than one program can be loaded to the main memory, but only one will use the CPU to execute instructions, inside this sort of environment programs that are not using CPU are waiting their turn, but off course this is not acceptable, imagine if the same program hold the CPU for 1h? To solve this problem, the OS will interrupt the current program as soon it starts to perform non CPU required operations, giving the control to one of the programs in memory ready to execute this process is a.k.a context switch. This way no CPU time is wasted waiting, e.g., I/O tasks to finish or to hope that the program will voluntarily release the CPU.\n Notice that at this point we\u0026rsquo;re talking about whole programs\n Multiprocessing #  To the end user multiprocessing may look similar to multiprogramming since we see multiple programs executing at the same time, but different of multiprogramming that perform the trick through context switch and memory segregation only, multiprocessing is more about CPU units, if the hardware provides more than one processor, more than one program will execute at the same time. Keep in mind that multiprocessing and multiprogramming are not exclusive. A system can use both mechanisms.\nMultitasking #  Multitasking is similar to what we have on multiprogramming, yet at this point we\u0026rsquo;re not just talking about programs, we are now talking about programs, processes, tasks, and threads running at the same time if we have more than one CPU or the illusion of \u0026ldquo;same time\u0026rdquo; through context switch if not, although once again, they are not exclusive. The difference now is that we may have small processes or even sub-tasks using the CPU in a fair way, using CPU time quantums defined by the scheduler instead allowing prosses to kidnap the CPU until it blocks.\nMultithreading #  Multithreading is a bit different from the previous models. The idea is that now we have multiple sub-segments \u0026ldquo;threads\u0026rdquo; running concurrently inside the context of another process, in other words, threads are child process sharing parent\u0026rsquo;s resources executing independently. This concept is interesting because in the previous model each process had its resource quota to perform a task, in a multithreading environment, the primary process children share parent\u0026rsquo;s resources so, a process can execute multiple computations in parallel through its children with the resources already available. But this advantage comes with some burden too. The programmer now needs to handle race conditions since now children are fighting for their parent\u0026rsquo;s resources. Well, well, well.\nSo what can we do to manage race conditions and prevent the system from ending up inconsistently? There are some approaches.\n Stop sharing state across threads Make state immutable Use synchronization whenever the shared mutable data happen  To start with, I would like to do another recap, now about the Java thread memory model: As you can see in the image above, Objects are kept in the Heap space and shared across all threads, do you remember the parent-child process idea? Java is the parent and threads the children, so Java threads have access to the parent resources, including the Heap space.\n JVMs can use a technique called escape analysis, by which they can tell that specific objects remain confined to a single thread for their entire lifetime, and that lifetime is bounded by the lifetime of a given stack frame. Such objects can be safely allocated on the stack instead of the heap.\n But Java threads have its private area, the thread stack, where local variables and partial method results live their lives.\nAnother important aspect when talking about threads is the CPU cache. In a multithreaded environment for performance reasons each thread may keep a copy of the variables inside the CPU cache, in this case, if the thread operating over the CPU A change a variable, there\u0026rsquo;s no guarantee that this change will be replicated back to the main memory so, another thread would be looking at an inconsistent value visibility problem.\n The mechanism available in Java to address visibility problems is the volatile keyword, declaring a variable volatile indicates that changes to that variable will be written back to the main memory immediately and read as well will happen directly from the main memory. But even volatile could not be enough, let\u0026rsquo;s suppose you need to read first to figure out the next value before writing something, you could end up with race conditions problems inside this small time gap.\nEnough recap let\u0026rsquo;s take a look in four (Thread confinement, Stack confinement, Object confinement and Immutability) different approaches to design a thread safe class. The one giving birth to that code is responsible for addressing race conditions internally, do you remember encapsulation? You apply in this case too. If you encapsulate your class correctly, you can change your race condition control if you have to without penalizing the ones using it.\nThere are many different scenarios we could use to exemplify race conditions situations, but let\u0026rsquo;s see one by dr kabutz. A basic util class used to format dates.\npublic class DateFormatter { private final DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\"); public String format(Date date) { return df.format(date); } public Date parse(String date) throws ParseException { return df.parse(date); } }  Thread confinement #  If we execute this code 1_000_000 against 4 threads, what do we have?\njava.util.concurrent.ExecutionException: java.lang.NumberFormatException: multiple points\nIt looks like DateFormat is not thread-safe. We could use ThreadLocal to confine our SimpleDateFormat into the current running thread.\n@ThreadSafe public class DateFormatter { private static final ThreadLocal threadLocal = ThreadLocal.withInitial( () - new SimpleDateFormat(\"yyyy-MM-dd\") ); public String format(Date date) { return threadLocal.get().format(date); } public Date parse(String date) throws ParseException { return threadLocal.get().parse(date); } }  Now we are safe, and the GC log looks like below.\ntime = 2038 durationOfLog=PT2.161S numberOfGCs=12 numberOfYoungGCs=12 numberOfOldGCs=0 memoryReclaimedDuringYoung=3.562GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.484MB totalMemoryAllocated=3.563GB averageCreationRate=1.65GB/s timeInGCs=PT0.015351S timeInYoungGCs=PT0.015351S averageTimeInYoungGCs=DoubleSummaryStatistics{count=12, sum=0.015351, min=0.000543, average=0.001279, max=0.002282} timeInOldGCs=PT0S percentageOfTimeInGC=0.71%  All right, I may be mistaken but internally ThreadLocal keeps a static class called ThreadLocalMap which keeps an Entry[] called table where Entry extends WeakReference. Again, I\u0026rsquo;m not sure of what I\u0026rsquo;m about to say, but I remember of posts where people were pointing this feature as source of problems like if ThreadLocal maps are not cleaned up properly after a transaction, the next TransactionProcessingTask might inherit values from another previous, unrelated task and there are even articles about of how to clean ThreadLocals so be careful.\nStack confinement #  Another solution would be stack confinement, do you remember the first image above? If not please take another look. What about this:\n The thread stack, it holds local variables and partial results, and plays a part in method invocation and return\n @ThreadSafe public class DateFormatter { public String format(Date date) { final DateFormat df = getDateFormat(); return df.format(date); } public Date parse(String date) throws ParseException { final DateFormat df = getDateFormat(); return df.parse(date); } private DateFormat getDateFormat() { return new SimpleDateFormat(\"yyyy-MM-dd\"); } }  Now we have local variables defined when method invocations happen, meaning that our DateFormat will live inside the thread stack and is not leaking to the heap.\nAnd this is how the GC log looks now.\ntime = 4599 durationOfLog=PT4.962S numberOfGCs=63 numberOfYoungGCs=63 numberOfOldGCs=0 memoryReclaimedDuringYoung=15.573GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.508MB totalMemoryAllocated=15.574GB averageCreationRate=3.14GB/s timeInGCs=PT0.0453345S timeInYoungGCs=PT0.0453345S averageTimeInYoungGCs=DoubleSummaryStatistics{count=63, sum=0.045335, min=0.000501, average=0.000720, max=0.002288} timeInOldGCs=PT0S percentageOfTimeInGC=0.91%  Again, 1_000_000 rounds against 4 lovable threads.\nEasier but twice slower and the total memory allocated was quite higher too. But still, safe and easy.\nObject confinement #  Another option is the object confinement, here we\u0026rsquo;re declaring DateFormat as a private final field and synchronizing the access to it avoiding two+ threads to have access to it at the same time.\n@ThreadSafe public class DateFormatter { @GuardedBy(\"this\") private final DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\"); public synchronized String format(Date date) { return df.format(date); } public synchronized Date parse(String date) throws ParseException { return df.parse(date); } }  Let\u0026rsquo;s see the GC log result:\ntime = 7307 durationOfLog=PT7.61S numberOfGCs=98 numberOfYoungGCs=98 numberOfOldGCs=0 memoryReclaimedDuringYoung=4.124GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.492MB totalMemoryAllocated=4.125GB averageCreationRate=555.06MB/s timeInGCs=PT0.0619821S timeInYoungGCs=PT0.0619821S averageTimeInYoungGCs=DoubleSummaryStatistics{count=98, sum=0.061982, min=0.000455, average=0.000632, max=0.002529} timeInOldGCs=PT0S percentageOfTimeInGC=0.81%  All right, this approach spare some memory allocation, but the execution was even worse than stack confinement. It may be because we\u0026rsquo;re introducing some contention synchronizing these methods.\nImmutability #  This one will look a bit unfair because address the specific problem related to the Date subject but you can abstract it and think about immutability applied to any other scenario.\nA new date API landed with Java 8, a useful class here would be DateTimeFormatter that by definition is thread-safe.\n A formatter created from a pattern can be used as many times as necessary, it is immutable and is thread-safe.\n @ThreadSafe public class DateFormatter { private static final DateTimeFormatter df = DateTimeFormatter.ISO_LOCAL_DATE; public String format(LocalDate date) { return df.format(date); } public LocalDate parse(String date) { return LocalDate.parse(date, df); } }  The GC log result once again, 1_000_000 executions and 4 threads.\ntime = 1371 durationOfLog=PT1.434S numberOfGCs=10 numberOfYoungGCs=10 numberOfOldGCs=0 memoryReclaimedDuringYoung=1.999GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.391MB totalMemoryAllocated=2.000GB averageCreationRate=1.39GB/s timeInGCs=PT0.0147572S timeInYoungGCs=PT0.0147572S averageTimeInYoungGCs=DoubleSummaryStatistics{count=10, sum=0.014757, min=0.000533, average=0.001476, max=0.002788} timeInOldGCs=PT0S percentageOfTimeInGC=1.03%  What do we have here, 1.3 second and just 2G of memory allocated, not bad at all, apparently Scala people have a point. Thread-safe, quick and cheap.\nConclusion #  The conclusion here is inconclusive :) The approach of choice will probably depend on what is more important to the scenario in hand, are you short in memory, do you care more about performance or readability? Don\u0026rsquo;t you care about any of these aspects and what matters to you is to deliver quick and let the problem blow in someone else\u0026rsquo;s hands? you know that there\u0026rsquo;s a special place in hell for you, right?\nDepending on what you\u0026rsquo;re planning to do about your afterlife or about the scenario you have in hands in this world, you should consider carefully the approach you\u0026rsquo;re going to use to address thread-safe, you can make it concurrent proof but you may introduce other problems if you don\u0026rsquo;t look to the options you have first.\nResults summary:\nThread confinement time = 2038 totalMemoryAllocated=3.563GB Stack confinement time = 4599 totalMemoryAllocated=15.574GB Object confinement time = 7307 totalMemoryAllocated=4.125GB Immutability time = 1371 totalMemoryAllocated=2.000GB References #   https://www.javaspecialists.eu/archive/Issue229.html https://gabrieletolomei.wordpress.com/miscellanea/operating-systems/virtual-memory-paging-and-swapping/ https://www.geeksforgeeks.org/operating-system-difference-multitasking-multithreading-multiprocessing/ https://medium.com/@unmeshvjoshi/how-java-thread-maps-to-os-thread-e280a9fb2e06 https://dzone.com/articles/java-memory-and-cpu-monitoring-tools-and-technique http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-2.html#jvms-2.5.2 http://tutorials.jenkov.com/java-concurrency/volatile.html\n"},{"id":8,"href":"/posts/0001-Websockets/","title":"Websockets","section":"Posts","content":"Olá!\nToday post is about a small experiment using Spring 5 to play with WebFux, I\u0026rsquo;ve used it to create a small WebSocket controller to \u0026ldquo;simulate\u0026rdquo; an e-mail inbox. The idea was to send some dummy text to the backend, persist it on MongoDB and from time to time check for new messages and send it to the clients connected to the WebSocket endpoint.\nThe Cast #  Let\u0026rsquo;s see an overview of all actors involved in this small experiment. This text probably will be much longer than the code necessary to have everything working with Spring 5.\nI\u0026rsquo;ll assume that Spring doesn\u0026rsquo;t need any introduction even because is hard to define it nowadays, if we were back to 2002 ish I would say it is an Ioc container blah blah blah but today\u0026hellip;\nMongoDB #   MongoDB is a free and open-source cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with schemata.\n MongoDB Reactive Streams Java Driver #   MongoDB Reactive Streams Java Driver, providing asynchronous stream processing with non-blocking back pressure for MongoDB.\n Project Reactor #   Non-Blocking Reactive Streams Foundation for the JVM both implementing a Reactive Extensions inspired API and efficient event streaming support.\n  The reactive design pattern is an event-based architecture for asynchronous handling of a large volume of concurrent service requests coming from single or multiple service handlers.\n  And the Spring Reactor project is based on this pattern and has the clear and ambitious goal of building asynchronous, reactive applications on the JVM.\n WebFlux #   Spring WebFlux is an asynchronous framework from the bottom up. It can run on Servlet Containers using the Servlet 3.1 non-blocking IO API as well as other async runtime environments such as netty or undertow.\n WebSocket #   WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection.\nThe WebSocket protocol enables interaction between a web client (such as a browser) and a web server with lower overheads, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the connection open. In this way, an ongoing two-way conversation can take place between the client and the server.\n The actual code #  pom.xml #  Let\u0026rsquo;s start with the pom.xml file, this way you\u0026rsquo;ll have what it takes to play by yourself with your WebSockets. Let\u0026rsquo;s check the most important parts.\nThe parent pom or the BOM file used to predefine the dependencies versions, so you can add your dependencies without having to figure out about version compatibility between the components.\n BOM stands for Bill Of Materials. A BOM is a particular kind of POM that is used to control the versions of a project’s dependencies and provide a central place to define and update those versions.\n  BOM provides the flexibility to add a dependency to our module without worrying about the version that we should depend on.\n \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.4.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt;  The next part, the dependencies I\u0026rsquo;ve used, observe that except for the most specific dependencies I didn\u0026rsquo;t configure any version, it\u0026rsquo;s all done by the parent pom.\n\u0026lt;dependencies\u0026gt; \u0026lt;!-- Boot --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-websocket\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-mongodb-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Tooling --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Websocket --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.webjars\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;webjars-locator-core\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.projectreactor\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;reactor-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-lang\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-lang\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.easytesting\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fest-assert\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  I won\u0026rsquo;t add the whole pom here. If you want, you can generate your one, indeed more updated from the SPRING INITIALIZR page. Pick any functionality you need for your project, and it will just generate a skeleton for you.\napplication.properties #  This one is a nice to have, especially if you\u0026rsquo;re going to use database connections, you can place your configurations here instead of having them hardcoded and spread through your code. In case you need extra configuration, and you don\u0026rsquo;t know for sure what property you need, you can check the Common application properties.\nFor the today\u0026rsquo;s experiment, what I needed was just some port definition, logging, and MongoDB URI connection.\nserver.port=8080 #logging logging.level.org.springframework.data=debug logging.level.=debug #mongodb spring.data.mongodb.uri=mongodb://r640:27017/chat  WebSocket configurations #  The code below is all you need to have your initial WebSocket configuration, I saw some more complex code essentially doing the same thing, but I suppose they were from previous versions of Spring, with Spring 5, that\u0026rsquo;s all you need to start.\n@Configuration @EnableWebSocketMessageBroker public class WebSocketConfig implements WebSocketMessageBrokerConfigurer { @Override public void configureMessageBroker(MessageBrokerRegistry config) { config.enableSimpleBroker(\"/topic\"); config.setApplicationDestinationPrefixes(\"/app\"); } @Override public void registerStompEndpoints(StompEndpointRegistry registry) { registry.addEndpoint(\"/email\"); registry.addEndpoint(\"/email\").withSockJS(); registry.addEndpoint(\"/emails\"); registry.addEndpoint(\"/emails\").withSockJS(); } }  The interesting points here are first the @EnableWebSocketMessageBroker annotation, here we\u0026rsquo;re telling Spring that you want to provide WebSockets capabilities on your application, config.enableSimpleBroker(\u0026quot;/topic\u0026quot;) tells spring that anything starting with /topic is a WebSocket endpoint where clients can subscribe to, config.setApplicationDestinationPrefixes(\u0026quot;/app\u0026quot;) again gives Spring some information, now it tells that clients will send data to your application through any endpoint starting with /app, registry.addEndpoint(\u0026quot;/email\u0026quot;) here we\u0026rsquo;re telling Spring about the endpoints we want to use the STOMP protocol, notice that we\u0026rsquo;re overriding registerStompEndpoints these endpoints will probably match some @MessageMapping on your controller.\nModel #  Here we\u0026rsquo;re primarily providing the definitions of how the MongoDB collection will look like, notice @Document and @Id annotations, they are basically saying to Spring that this class is about collection entries called email identified by the id field, if we want the collection to have a different name we could define Document like @Document(collection = \u0026quot;another_name\u0026quot;)\n@Data @Builder @Document public class Email { @Id private String id; private String content; private final Date sentAt = new Date(); }  Repository #  This part is surprising if there\u0026rsquo;s no need for anything special from the repository, the only thing needed is an interface extending ReactiveMongoRepository, Spring will provide basic operations out of the box.\nTo get just the newest emails since the last verification all we need is to provide an abstract method with a @Query annotation, and anything else is handled by Spring, what a wonderful world we\u0026rsquo;re living these days :)\n@Repository public interface EmailRepository extends ReactiveMongoRepository{ @Query(\"{ 'sentAt' : { $gte : ?0 }}\") Flux findLastOnes(Date lastExecution); }  The controller #  Please ignore the private Date lastExecution = new Date() over there, by default Spring creates singleton beans so we\u0026rsquo;re safe here and please, stop thinking about a service layer if you\u0026rsquo;re doing so, we don\u0026rsquo;t need it here right now.\n@Controller @EnableScheduling public class EmailController { @Autowired private EmailRepository repository; @Autowired private SimpMessagingTemplate template; private Date lastExecution = new Date(); @MessageMapping(\"/email\") public void email(final Email message) { repository.save(message).block(); } @MessageMapping(\"/emails\") public void emails() { repository.findAll().subscribe(new EmailSubscriber()); } @Scheduled(fixedRate = 10000) public void updateClients() { repository.findLastOnes(lastExecution).subscribe(new EmailSubscriber()); } private class EmailSubscriber extends BaseSubscriber { @Override protected void hookOnComplete() { lastExecution = new Date(); } @Override protected void hookOnError(Throwable throwable) { template.convertAndSend(\"/topic/email/errors\", throwable.getMessage()); } @Override protected void hookOnNext(T value) { final Email email = (Email) value; email.setContent(HtmlUtils.htmlEscape(email.getContent())); template.convertAndSend(\"/topic/email/updates\", email); } } }  Now, the interesting parts here, the @MessageMapping annotated methods are the ones that clients will send requests too, just remember that from the client side, they need to hit for example /app/email instead of just /email, do you remember WebSocketConfig, config.setApplicationDestinationPrefixes(\u0026quot;/app\u0026quot;)?\nTo simulate that small delay from the point when someone sends us an email and the moment we receive it, I\u0026rsquo;ve configured an schedule that runs every 10 seconds, take a look at updateClients. Here some interesting things are going on. First, we\u0026rsquo;re asking the repository to give us the new messages since the last verification and, instead of wait for the answer, we\u0026rsquo;ve subscribed, furthermore when something comes back we\u0026rsquo;ll be notified so we can push content to the connected clients when and just when we have something to do so.\nsubsctibe is a method provided by Flux that in the Reactor world represents a Reactive stream of size from 0 to N, for 0 or single result Mono is the right representation. Ok, we have some results, what happens now? The Subscriber hookOnNext method will be executed once for each of the found results. Here we can do some work like escape characters. Because we need at least one method with more than one line right? From this point just sent it to the proper topic and the subscribed clients will be updated.\nTesting #  Testing it was simple too since with a simple @SpringBootTest annotation the whole stack was made available, we only have to do some initial setup to have an integration test up and running.\n@RunWith(SpringJUnit4ClassRunner.class) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT) public class EmailControllerTest { private static final String SUBSCRIBE_TOPIC_ENDPOINT = \"/topic/email/updates\"; private static final String SEND_EMAIL_ENDPOINT = \"/app/email\"; @Autowired private EmailRepository repository; @Autowired private ReactiveCrudRepository operations; @Value(\"${local.server.port}\") private int port; private String URL; private CompletableFuture completableFuture; private WebSocketStompClient stompClient; private StompSession stompSession; @Before public void setup() throws InterruptedException, ExecutionException, TimeoutException { completableFuture = new CompletableFuture(); URL = String.format(\"ws://localhost:%d/email\", port); stompClient = new WebSocketStompClient(new SockJsClient(createTransportClient())); stompClient.setMessageConverter(new MappingJackson2MessageConverter()); stompSession = stompClient.connect(URL, new StompSessionHandlerAdapter() { }).get(1, TimeUnit.SECONDS); } @Test public void receiveEmails() throws InterruptedException, ExecutionException, TimeoutException { // Adding a dummy email final Email email = Email .builder() .id(UUID.randomUUID().toString()) .content(RandomStringUtils.randomAlphabetic(10)) .build(); // Send the email through webSocket to be persisted stompSession.send(SEND_EMAIL_ENDPOINT, email); // Subscribing to the notification endpoint as a client stompSession.subscribe(SUBSCRIBE_TOPIC_ENDPOINT, new MessageStompFrameHandler()); // Waiting up to 10s for a notification to match the scheduler final Email emailReceived = completableFuture.get(10, TimeUnit.SECONDS); // Validating content assertThat(emailReceived).isNotNull(); assertThat(emailReceived.getId()).isEqualTo(email.getId()); assertThat(emailReceived.getContent()).isEqualTo(email.getContent()); assertThat(emailReceived.getSentAt()).isEqualTo(email.getSentAt()); // Removing dummy operations.delete(email).block(); // Verifying if is really gone final Email foundEmail = repository.findById(email.getId()).block(); assertThat(foundEmail).isNull(); } private List createTransportClient() { final List transports = new ArrayList(1); transports.add(new WebSocketTransport(new StandardWebSocketClient())); return transports; } private class MessageStompFrameHandler implements StompFrameHandler { @Override public Type getPayloadType(StompHeaders stompHeaders) { return Email.class; } @Override public void handleFrame(StompHeaders stompHeaders, Object o) { completableFuture.complete((Email) o); } } }  The client #  To play here I\u0026rsquo;ve wrote a html, js and css files, placed them inside the src/main/resources/static folder and started the project, when hitting localhost:8080 the client gets connected automatically, and the fun is complete, I\u0026rsquo;m not proud of them, but you can have them index.html, index.js and index.css.\nConclusion #  Once again Spring did an excellent job to provide ways to write applications with low effort. With few lines of code, we can see a new paradigm acting over every single layer of our application. And in case you\u0026rsquo;re wondering if it could be applied somewhere else but WebSockets related endpoints, you can use mainly the same dependencies and approach to see similar results happening for example for rest endpoints.\nHope it provided you a glance of what\u0026rsquo;s possible to achieve with this new Spring version and how much fun you can have playing around with these new toys :)\nReferences #   Spring Boot\n Spring 5\n WebFux\n MongoDB\n WebSocket\n WebSocket Support on Spring 5\n Stomp\n Project Reactor\n Reactive Streams\n Spring Initializr\n Common Spring application properties\n"}]