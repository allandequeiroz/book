[{"id":0,"href":"/posts/0008-Formulae/","title":"Formulae","section":"Posts","content":"$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$\nExpand\rColumn 1 Column 2 Column 3 Column 4 Data 1.1 $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Data 1.3 Data 1.4 Data 2.1 Data 2.2 Data 2.3 Data 2.4 Data 3.1 Data 3.2 $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Data 3.4 "},{"id":1,"href":"/posts/0007-A-parte-final/","title":"A parte final","section":"Posts","content":" A parte final # 11 de dezembro de 2015 Por Tim Urban\nEm uma postagem no ano passado, apresentamos visualmente a expectativa de vida humana. Por anos:\nPor meses:\nE por semanas:\nEnquanto trabalhava nesse post, também fiz um gráfico de dias, mas parecia um pouco demais, então deixei de fora.\nO gráfico dos dias me surpreendeu tanto quanto o gráfico das semanas. Cada um desses pontos representa apenas uma terça, sexta ou domingo, mas mesmo uma pessoa sortuda que vive até os 90 anos não terá problemas em encaixar todos os dias de sua vida em [uma folha de papel]( https://waitbutwhy.com/wp -content/uploads/2015/12/A-90-Year-Life-in-Days.pdf).\nMas desde que escrevi o post Life in Weeks, tenho pensado em outra coisa.\nEm vez de medir a sua vida em unidades de tempo, você pode medi-la em atividades ou eventos. Por exemplo:\nTenho 34 anos, então vamos ser super otimistas e dizer que ficarei desenharei bonequinhos até os 90 anos. 1 Se for assim, ainda tenho pouco menos de 60 invernos:\nE talvez eu ainda consiga assitir 60 Superbowls:\nO oceano está congelando e colocar meu corpo nele é uma experiência de vida ruim, então tendo me limitar a nadar no oceano gelado uma vez por ano. Por mais estranho que pareça, talvez eu só entre no oceano mais 60 vezes:\nLeitura, eu leio cerca de cinco livros por ano, então mesmo que eu tenha a sensação de que vou ler um número infinito de livros no futuro, na verdade tenho que escolher apenas 300 de todos os livros que existem para ler e aceitar que vou morrer sem saber o que acontece em todo o resto.\nHouveram oito eleições presidenciais nos EUA durante a minha vida e faltam cerca de 15. Já vi cinco presidentes no cargo e, se esse ritmo continuar, verei mais nove.\nEu como pizza uma vez por mês, então tenho cerca de 700 chances a mais de comer pizza. Tenho um futuro ainda melhor com bolinhos. Eu como comida chinesa duas vezes por mês e junto, eu peço seis bolinhos, então tenho uma boa quantidade de bolinhos pela frente:\nMas essas não são as coisas nas quais eu estou realmente pensando. A maioria das coisas que acabei de mencionar acontecem com uma frequência semelhante durante cada ano da minha vida, o que as distribui de maneira um tanto uniforme ao longo do tempo.\nO que tenho pensado é uma parte realmente importante da vida que, ao contrário de todos esses exemplos, não está distribuída uniformemente ao longo do tempo – algo cuja proporção [já feito/ainda por vir] não se alinha de forma alguma com a forma como longe estou na vida:\nRelacionamentos.\nTenho pensado em meus pais, que estão na casa dos 60 anos. Durante meus primeiros 18 anos, passei um bom tempo com meus pais durante pelo menos 90% dos meus dias. Mas desde que fui para a faculdade e depois saí de Boston, provavelmente os vi em média de 10 dias por ano. Cerca de 3% dos dias que passei com eles em cada ano da minha infância.\nEstando na casa dos 60 anos, vamos continuar super otimistas e dizer que sou uma das pessoas incrivelmente sortudas por ter ambos os pais vivos até os meus 60 anos. Isso me daria cerca de mais 30 anos de convivência. Se a frequencia de dez dias por ano se mantiver, eu só terei a companhia de meus pais por mais 300 dias. Menos tempo do que passei com eles em qualquer um dos meus 18 anos de infância.\nAo olhar para essa realidade, você percebe que, apesar de não estar no fim da sua vida, pode muito bem estar chegando ao fim do seu tempo com algumas das pessoas mais importantes da sua vida. Se eu definir o total de dias que passarei com meus pais - presumindo que tenho a maior sorte possível - isso fica totalmente claro:\nAcontece que quando me formei no ensino médio, eu já havia usado 93% do meu tempo presencial com meus pais. Agora estou aproveitando os últimos 5% desse tempo. Estamos no final da nossa convivência.\nÉ uma história semelhante com minhas duas irmãs. Depois de morar em uma casa com elas por 10 e 13 anos respectivamente, moro agora do outro lado do país longe das duas com quem convivo talvez 15 dias por ano em pessoa. Esperançosamente, isso nos deixa com cerca de 15% do tempo total restante.\nO mesmo acontece frequentemente com velhos amigos. No ensino médio, eu ficava jogando copas com os mesmos quatro melhores amigos, cinco dias por semana. Em quatro anos, provavelmente acumulamos 700 encontros em grupo. Agora, espalhados pelo país com vidas e horários totalmente diferentes, nos encontramos em uma média de 10 dias a cada década. O grupo está nos 7% finais.\nEntão, o que fazemos com essas informações?\nDeixando de lado minha esperança secreta de que os avanços tecnológicos me permitirão viver até os 700 anos, vejo aqui três conclusões:\nMorar no mesmo lugar que as pessoas que você ama é importante. Provavelmente passo 10 vezes mais tempo com as pessoas que moram na minha cidade do que com as pessoas que moram em outro lugar.\nAs prioridades são importantes. O tempo que resta com qualquer pessoa depende muito de onde essa pessoa se enquadra na sua lista de prioridades de vida. Certifique-se de que esta lista seja definida por você – e não por inércia inconsciente.\nTempo de qualidade é importante. Se você está nos últimos 10% do tempo com alguém que ama, mantenha esse fato em mente quando estiver com essa pessoa, trate esse tempo como o que realmente é: precioso.\n"},{"id":2,"href":"/posts/0006-Framing-and-Reframing/","title":"Framing and Reframing","section":"Posts","content":" Problem Statement # In the wake of unexpected leadership changes at AnotherInc Inc., Ethan, a previously successful vice president, faces significant challenges stemming from the company’s altered dynamics alongside his diminished recognition and estrangement from relevant work. These challenges have been underscored by a shift in power structures and a noticeable change in professional attitudes towards Ethan by the new leaders, Marcus and Daveed. The situation has further escalated due to what he perceives as unfair evaluations, further exacerbating his marginalisation within the organisation.\nSymptoms # Ethan\u0026rsquo;s exclusion from a key role, despite his contributions and seniority, points to potential favouritism or unfair treatment. Ethan\u0026rsquo;s professional disagreements with Marcus and Daveed, followed by their subsequent shift in attitudes towards him, signal the presence of interpersonal conflicts and poor communication. Ethan receiving a poor evaluation despite his achievements and mentorship efforts suggests potential bias in the performance appraisal process. Ethan\u0026rsquo;s exclusion from important roles and relevant work, leading to another poor evaluation, indicates the possibility of intentional sidelining or workplace bullying. Protagonists # Direct: # Ethan – Vice President of Software Engineering Daveed – Director of Software Engineering Marcus – Managing Director and Head of Fixed Income Investments Engineering Indirect: # Antonio – Former Managing Director and Head of Fixed Income Investments Engineering Kiko – Former Director of Software Engineering Background # AnotherInc Inc. is globally recognised as a leading investment management firm, specialising in areas such as risk management, strategic advisory and enterprise investment system services. Within the company, the environment is viewed from two distinct perspectives, one as a dynamic, challenging and rewarding environment, while the other is that of a high-pressure workplace demanding long hours and rigorous commitment.\nIn 2019, Ethan joined AnotherInc as vice president, leading a software initiative to address a common challenge pervasive within the financial industry. Following two successful years and outstanding performance reviews, he transitioned to the Fixed Income Investments business, where he worked directly with two widely recognised and respected figures within the corporation, Antonio and Kiko. One of Ethan\u0026rsquo;s first tasks was building a new software engineering team for a recently approved three-year project. However, during the initial phases, and with a few team members already onboarded, Antonio and Kiko abruptly left AnotherInc.\nMarcus, who had previously held a senior position in Jafar\u0026rsquo;s Mutual Funds engineering, was announced as Antonio\u0026rsquo;s successor in the role of head of Fixed Income Investments engineering. On his appointment, he immediately began to rework the initiatives Antonio had previously set in motion. These modifications affected everyone on the team, including Ethan, who was mentoring new hires at that time. The dynamics took a further turn for the worse when Marcus overlooked Ethan, who was next in line following Kiko\u0026rsquo;s departure, and instead appointed Daveed, someone Marcus had previously hired to work alongside him in the Mutual Funds business.\nThe professional relationships that Ethan had previously fostered with Antonio and Kiko were straightforward and open, facilitating fluid communication. However, things changed under Marcus and Daveed, a shift Ethan initially failed to recognise. On two separate occasions, Ethan publicly shared his professional opinions, which contradicted those of Marcus and Daveed. These interactions were not well-received, leading to a significant shift in their attitudes towards Ethan.\nDespite Ethan mentoring the entire new team and exceeding the goals set by his former manager, he received a poor evaluation at the end of his first year working under Marcus and Daveed. In the subsequent year, Ethan was perceived as further isolated from his previous significant roles, distancing him from all relevant activities. This situation virtually erased Ethan\u0026rsquo;s presence, setting the stage for another possible poor evaluation.\nWith this introduction, I will now move to my analyses, where I intend to use the political frame to discuss the possible overuse of power and its potentially profound consequences. In the human resources frame, I will explore differing interpretations of the problem where both lead to the same outcome despite the analyses contracting each other.\nPolitical Frame # What makes the political frame particularly suitable for this organisational problem is the previously existing coalition formed by Marcus and Daveed. This originated from their time at Mutual Funds, later morphing into collusion in their dealings with Ethan. The evident power dynamics and political manoeuvrings aimed to establish them as new figures of power after Antonio and Kiko\u0026rsquo;s departure. Both had been charismatic figures (King and Lawley, 2022, p. 618), Kiko exerting power via legitimate and expert sources and Antonio exercising power originated from legitimate, reward, referent and informal sources (King and Lawley, 2022, p. 617).\nI would like to commence my analysis from a standpoint where power is viewed as malleable and adaptable; its characterisation as good or evil depends on the outcome envisioned by the power holder(s) in dealing with a situation, whether they be groups or individuals. Foucault (2020, p. 194) states that \u0026lsquo;[…] power produces; it produces reality; it produces domains of objects and rituals of truth\u0026rsquo;. In addition, Bolman and Deal (2021) noted that a manager\u0026rsquo;s credibility is highly important, and Ethan\u0026rsquo;s public dissenting opinions were not necessarily favourable to Marcus\u0026rsquo;s and Daveed\u0026rsquo;s momentum, where both were striving to establish themselves as suitable replacements for previously widely accepted leadership figures, namely Anthony and Kiko.\nSo, what could possibly justify such drastic measures against Ethan? It is hard to conceive that it was simply about contradictory opinions. My analysis points towards the public expectations regarding Marcus and Daveed’s capabilities to fill the void left by the previous managers, the timing, and the type of power held by each individual (Bolman and Deal, 2021; King and Lawley, 2022). Due to Ethan\u0026rsquo;s involvement with a wide range of business areas and technologies across the firm, his responsibilities and influence grew beyond his title. Korda (1976, p. 123) notes, \u0026lsquo;instead of moving upward, he expands outward\u0026rsquo;, granting him an expert-type base of power where in a competitive environment such as AnotherInc, such a power base is naturally accompanied by a referent base type of power generated by good publicity and public rewards. Contrastingly, Marcus and Daveed are very aligned in their characteristics – both are hands-off, introspective managers who intelligently combine hierarchical legitimacy with reward bases of power.\nKorda (1976) argued that \u0026lsquo;friction works against progress\u0026rsquo; and Ethan\u0026rsquo;s failure to recognise Marcus\u0026rsquo;s and Daveed\u0026rsquo;s individual traits combined with a less than ideal situation, naturally begotten of a transitory condition where their power was yet to be established and recognised, alongside the unsurprising attention that members of a group afford newcomers, laid the foundation for collusion. Thus Ethan became a timely target for a display of power, one which was also useful in seeking to deter others from following suit (Foucault, 2020, p. 93).\nWhat perhaps makes the situation complicated for Marcus and Daveed is that Ethan remains a respectable figure within the community, rendering any direct, immediate, and hard type of punishment inappropriate. What subsequently unfolds is what Foucault (2020) describes as \u0026rsquo;the gentle way in punishment\u0026rsquo;, where the English model adds isolation as an extra means for correction. Using their influence and decision-making power, Marcus and Daveed divested Ethan of any significant duties, beginning by promoting less experienced professionals as go-to people, transferring Ethan\u0026rsquo;s responsibilities to others, excluding him from relevant meetings, or preventing him from speaking when present, withholding crucial internal opportunity information, and sabotaging even minor tasks he undertook, effectively reducing him to a standstill in isolation.\nIn keeping with Foucault\u0026rsquo;s (2020) concept of \u0026rsquo;the gentle way in punishment\u0026rsquo;, the castigation must be proportionate to the problems caused and should not be perpetual. For instance, it should not lead to Ethan\u0026rsquo;s dismissal or be prolonged to the point where punishment becomes a health problem. By doing so, the punisher risks becoming not only a wrongdoer they also become an evildoer, Manville et al. (2016) argued that acts of injustice are directly correlated with the victim\u0026rsquo;s negative health impacts: \u0026lsquo;injustice at work is seen as a job stressor that over time indirectly leads to MSD by eliciting negative emotional states, unhealthy behaviours and pathophysiological changes\u0026rsquo; (Ganster and Rosen, 2013; Robbins et al., 2012, as cited in Manville et al., 2016, p. 1317).\nThis section has attempted to analyse the problem of Marcus and Daveed\u0026rsquo;s potentially coercive and manipulative power usage. The main weakness is that by no means this is the only possible way to evaluate the presented situation rendering this analysis one-sided where Marcus and Daveed are seen as judge, jury, and executioners whilst Ethan is a victim. In my following analyses, using the Human Resource frame, I will attempt to look not only from this stance, I will also present a contrasting idea where Ethan is potentially the one driving the chain of events.\nHuman Resource Frame # As was pointed out in the introduction, with the advent of new leadership, the bases of power shifted. Despite Ethan\u0026rsquo;s recognised expertise, Marcus positioned Daveed as Kiko\u0026rsquo;s replacement. This decision might have clashed with Ethan\u0026rsquo;s expectations of becoming Kiko\u0026rsquo;s successor, potentially leaving him feeling diminished. Maslow\u0026rsquo;s Hierarchy of Needs (1943; 1954) and Herzberg\u0026rsquo;s Two-Factor Theory (1959; 1966) provide insight into the impact of these changes on Ethan\u0026rsquo;s motivation. Initially driven by esteem needs and motivational factors, such as achievement and recognition, Ethan found himself in a situation where these needs were no longer being met. Instead, the changes implemented by Marcus and, subsequently, Daveed created job dissatisfaction. They potentially threatened Ethan\u0026rsquo;s sense of job security (Herzberg, 1966), leading to a process of quiet quitting, a \u0026lsquo;diminishment of commitment and engagement to carry out tasks not specified in their job description\u0026rsquo; (Formica and Sfodera, 2022).\nAn alternative interpretation of the situation posits that Ethan may not have seen the condition as humiliating instead, he may have approached it from a position of superiority. As Gianpetro (2016, p. 3) notes, \u0026lsquo;when wishes do not materialise, we hardly ever blame ourselves for being irrationally hopeful. Instead, we blame the leader for not being good enough\u0026rsquo;. This perspective supports the notion that Ethan might have seen Daveed\u0026rsquo;s appointment as Kiko\u0026rsquo;s replacement as a failure of judgement by Marcus by not properly evaluating Ethan\u0026rsquo;s abilities to take Kiko\u0026rsquo;s place. Alternatively, he may have perceived the situation as a failure of character on Marcus\u0026rsquo;s part, believing the appointment was made based on a pre-existing alliance, not on merit. In this scenario, Ethan could have compared himself to Daveed and deemed himself more deserving of the position (Festinger, 1954), perceiving Marcus\u0026rsquo;s leadership as questionable and potentially using it as a pretext to remove himself and, consequently, his knowledge from Daveed\u0026rsquo;s range of options.\nThe argument presented by Gianpetro (2016, p. 3), where one blames leaders when wishes do not materialise, is also supported by the \u0026lsquo;predictable six steps that an individual may pursue in order to influence others\u0026rsquo;, as identified by Bolman and Deal (2021, p. 167) within Argyris and Schön’s Model I (henceforth Model I). In the first step, Ethan could have assumed that the problems he experienced originated from Marcus’s decision to bring in Daveed to replace Kiko and that this offer was based on Marcus’s faulty judgement or even lack of character, a theory that Ethan developed and could be used as support for his subsequent actions, as explained in the Model I second step. In the third step, Ethan withdrew himself from relevant activities, potentially with the intent to manipulate the situation by impacting Daveed’s performance as a manager. The fourth step is, in theory, described by Daveed’s resistance to holding on to his post, possibly interpreted by Ethan as due to Daveed’s alliance with Marcus, which Ethan would possibly see as a collusion formed with ill intent towards himself. In the fifth step, Ethan intensifies his actions to the point of self-marginalisation with little or no effect on Marcus and Daveed, driving Ethan towards the sixth step of Model I, which could be explained by the idea that Marcus and Daveed are protecting themselves while sabotaging Ethan.\nDiagnosis # The shift in power dynamics, with Marcus favouring Daveed over Ethan for Kiko\u0026rsquo;s position, triggered feelings of injustice and devaluation. The change in communication styles and lack of open dialogues under the new leadership exacerbated conflicts, leading to Ethan\u0026rsquo;s professional isolation. The shift from a culture of open communication and merit recognition to one that appeared to favour personal alliances resulted in decreased job satisfaction and demotivation, particularly for Ethan. Ineffective human resources practices, such as unfair performance evaluations and exclusion from vital work, were employed as punitive measures against Ethan. Alternative Solutions # Several possible alternatives, perhaps more relevant to the problem originating from the structural and symbolic frames, were not considered, likewise theories, concepts and models from the political and human resource perspectives were not explored. These limitations significantly impact my arguments. It is also worth mentioning that I have chosen to restrict the proposed solutions to simple and conciliatory options.\nOne possibility is that the unfulfilled needs perceived not only by Ethan as well by Marcus and Daveed are rooted in cultural differences, possibly deeply ingrained. As Greenberg (1990, p. 21) argued, it is \u0026rsquo;the collective programming of the mind which distinguishes the members of one human group from another\u0026rsquo;. This could include geographical differences or even the distinct mindset brought forward by the different subgroups presented here, where King and Lawley (2022, p. 789) suggest that \u0026lsquo;subculture is a localised culture with its own set of values and behaviours that reflects but is distinct from, the wider culture\u0026rsquo;.\nAnother solution to explore is bargaining and compromise. Bolman and Deal (2021) argue that, in organisations, \u0026lsquo;conflict is rampant due to enduring differences in needs and perspectives and that bargaining, negotiation, coercion and compromise are a normal part of everyday life\u0026rsquo;. Ethan could offer to compromise with Marcus and Daveed. This could involve agreeing to change his behaviour or agreeing to disagree on certain issues. While this conversation could be challenging, it could also be an opportunity for Ethan to reach a resolution with Marcus and Daveed before deeming the situation unworthy of further spent time and energy. If successful, this could lead to a solution. Alternatively, Ethan might consider transferring to another area or resigning.\nFinal Recommendations # Marcus, Daveed and Ethan should seek enhancement of their intercultural competencies via the cultural intelligence framework (Ang, Van Dyne and Koh, 2006). Ethan should communicate his concerns with Marcus and Daveed directly in private. Ethan could discuss his roles and responsibilities with Marcus and Daveed to ensure clarity and alignment with their expectations. Ethan should take steps to address the potential negative effects of his situation on his mental health. This could include seeking support from an employee assistance programme or considering a \u0026lsquo;job redesign\u0026rsquo; (Bolman and Deal, 2021, p. 153). Reference # Ang, S., Van Dyne, L. and Koh, C. (2006). Personality correlates of the four-factor model of cultural intelligence. Group \u0026amp; Organisation Management, 31(1), pp. 100–123.\nBacharach, S. B. and Lawler, E. J. (1980), Power and Politics in Organisations: The Social Psychology of Conflict, Coalitions, and Bargaining. San Francisco; London: Jossey-Bass.\nBolman, L. G. and Deal, T. E. (2021), Reframing Organisations: Artistry, Choice, and Leadership. 7th ed. San Francisco: Jossey-Bass.\nFestinger, L. (1954) . A theory of social comparison processes. Human Relations (New York), 7(2), pp. 117–140.\nFormica, S. and Sfodera, F. (2022). The great resignation and quiet quitting paradigm shifts: An overview of current situation and future research directions. Journal of Hospitality Marcuseting \u0026amp; Management, 31(8), pp. 899–907.\nFoucault, M. and Sheridan, A. (2020). Discipline and Punish: The Birth of the Prison. London: Penguin Classics.\nFrench, J. R., Raven, B. and Cartwright, D. (1959). The bases of social power. Classics of Organisation Theory, 7, pp. 311–320.\nGianpietro, P. (2016). Why we pick leaders with deceptively simple answers. [online] Harvard Business Review Digital Articles, May*,* pp. 2–4. Available from: https://hbr.org/2016/05/why-we-pick-leaders-with-deceptively-simple-answers#:~:text=Anxiety.\u0026amp;text=To%20distressed%20people%20in%20troubled,rise%20is%20never%20good%20news. (Accessed: 09 July 2023).\nGreenberg, J. (1990). Organisational justice: Yesterday, today, and tomorrow. Journal of Management, 16(2), pp. 399–432.\nHerzberg, F., Mausner, B. and Snyderman, B. B. (1959). The Motivation to Work. London; New York: J. Wiley and Sons.\nKing, D. and Lawley, S. (2022), Organisational Behaviour. 4th ed. Oxford: Oxford University Press.\nKorda, M. (1976). Power!: How to Get It, How to Use It. New York: Bethantine Books.\nManville, C., Akremi, A. E., Niezborala, M. and Mignonac, K. (2016). Injustice hurts, literally: The role of sleep and emotional exhaustion in the relationship between organisational justice and musculoskeletal disorders. Human Relations (New York), 69(6), pp. 1315–1339.\nMaslow, A. H. (1943). A theory of human motivation. Psychological Review, 50(4), pp. 370–396.\nMaslow, A. H. (1954). Motivation and Personality. New York: Harper.\nScheyett, A. (2022),Quiet quitting. Social Work (New York), 68(1), pp. 5–7.\nZaleznik, A. (1970). Power and politics in organisational life. Harvard Business Review, 48(3), pp. 47-60.\n"},{"id":3,"href":"/posts/0005-Home-made-Kubernetes-cluster/","title":"Home made Kubernetes cluster","section":"Posts","content":" Homemade Kubernetes Cluster # In my previous post, I wrote about how to set up a scalable Jenkins using Kubernetes at home on top of home devices, in my case, 3 Raspberries, 2 Rock64 and a NUC. Since it wasn\u0026rsquo;t a 5 minutes task I\u0026rsquo;ll describe here the steps I went through to get it working.\nPre-checks # Before installing Kubernetes you better check which Docker version is supported by the Kubernetes version you intend to install, a few days ago I upgraded my Kubernetes cluster from version 12 to 15 so, version 15 is what I\u0026rsquo;ll be used as a reference here.\nIf you go to the Kubernetes Changelogs page and look for Docker you\u0026rsquo;ll spot the \u0026ldquo;validated docker versions\u0026rdquo;, as we can see, I could have upgraded to the version 18.09, but in the end, I\u0026rsquo;ve decided by 18.6.2.\nSaid that before installing Docker and Kubernetes it might be a good idea to upgrade your whole system if it is safe to do so.\nDocker # Once you determined which Docker version to install, before moving forward you need to keep a few details in mind such as the device architectures you also have the OS version. You may need to edit these details at your \u001dsources.list\u001d otherwise the downloaded packages won\u0026rsquo;t work.\nIf you have any device which the architecture is not amd64, remove [arch=amd64] from sources.list. Instead of $(lsb_release -cs) you can specify the OS release you want to target by hand if you want, eg: xenial. Package version may look strange but it\u0026rsquo;s correct, eg: docker-ce=18.06.2~ce~3-0~ubuntu\u001c. Once you\u0026rsquo;re done with the steps above you can add the repository keys so you won\u0026rsquo;t get any harmful package by accident.\nNow is time to include the sources list per se, please notice the two options below, the first one is for anything that is not ARM, the second one for ARM architectures.\u001c\nAnything but ARM\nARM\nInstall Docker\nSetup Docker daemon\nRestart Docker\nAt this point, we should have a Docker version fully compliant with Kubernetes, up and running.\nDocker Compose # Since we\u0026rsquo;re installing Docker, just in case you decide to spin up a docker-compose for some reason, let\u0026rsquo;s install it as well. The points of attention are similar to the one used during the Docker installation.\u001c\nAnything but ARM\nARM\nThe process to have docker-compose at the ARM platform was a bit more complicated, at least for me, but is not so complicated, here are the steps I went through:\u001c\u001c\nDocker Compose bash autocompletion # Now that you have docker-compose everywhere is time to set the autocompletion to spare a bit of your patience down the road with typing.\u001c\nKubernetes # With a fully happy Docker running across all the devices, it is time to install Kubernetes.\nInitialising Master Kubernetes # This step will be easier if you save the script below at your master host, execute, and let it do its job. You can run step-by-step by hand if you want, just like the ones before, this is up to you.\nBy the end you\u0026rsquo;ll see amongst the logged lines, one starting with kubeadm join, copy this whole line and execute across the other devices you have. The line I\u0026rsquo;m talking about is similar to this:\u001c\nkubeadm join 192.168.1.2:6443 --token zd07uq.d91cxeg22nhwl6ti --discovery-token-ca-cert-hash sha256:551f848676f99621bbed06810d15532f69019398d18d475a0cbaa9e69ee9d195\nAt this point you should have all your devices communicating with the manager, to check how is it going execute kubectl get nodes -o wide at the manager node, maybe the nodes are not ready yet, give it some time, they need to perform some tasks as well. Once you see something like the lines below, you\u0026rsquo;re ready to go.\nKubernetes autocompletion # Last but not least, kubectl completion, believe me, this one is a must-have, especially about the Pod\u0026rsquo;s names, you won\u0026rsquo;t want to copy and paste or even worse, type the names Kubernetes gives to the Pods.\necho \u0026ldquo;source \u0026lt;(kubectl completion bash)\u0026rdquo; \u0026raquo; ~/.bashrc\nConclusion # It was a short step-by-step on how to set up your Kubernetes cluster; I believe it\u0026rsquo;ll be useful, especially if you have devices with different architectures. It may not be fully compatible with the scenario, but for sure it\u0026rsquo;ll give you some ideas to solve your problems.\nAnother thing, if you\u0026rsquo;re looking to go beyond and have your bare metal Load balanced cluster, take a look at MetalLb. And do not forget to take a look at the Cluster Networking page as well, the kubernetes_master.sh installed Wave Net for you but, you may want a different one.\nHope it helps!\n"},{"id":4,"href":"/posts/0004-Terraform/","title":"Terraform","section":"Posts","content":"TLDR; A walk through an EC2 instance setup, dynamically attaching volumes snapshots and configuring Cloudflare DNS entries pointing to the new instance using Terraform and Docker (Ghost, Traefik).\nSource code: https://github.com/allandequeiroz/blog/tree/extract_variables\nBasic infrastructure configuration using Terraform, Docker (Ghost, Traefik) and Cloudflare. # For a while, I\u0026rsquo;ve hosted this blog, at home, using Docker and Kubernetes across a few Raspberries (I use it as an excuse to do play around). It turns out that I\u0026rsquo;m moving to a new place, so I have a new excuse to play with something else.\nThe idea at this time is simply to provide the infrastructure on EC2 using Terraform, set up the DNS entries and keep the blog alive, later on, I\u0026rsquo;ll see what can I do with Kubernetes and perhaps include some load-balancing and autoscaling as well (even completely unnecessary due the low traffic) but for now let\u0026rsquo;s take a look at the current configurations:\nDocker (docker-compose) Ghost Traefik Traefik (toml) Terraform EC2 Cloudflare Docker # Ghost and Traefik # Different from my previous setup, at this time I\u0026rsquo;ve decided to move out from my customized Ghost version and start using a vanilla one provided by Ghost\u0026rsquo;s maintainers available on Docker hub, the only thing to keep was the config.production.json file to set details such as port, database and content placement; if you want to check this file, it\u0026rsquo;s available at the repository.\nAlso, to play with something new, I\u0026rsquo;ve replaced NGINX by Traefik; it was quite interesting to know that Traefik deals with all the underline details also is quite dynamic in terms of self-reconfiguration, we give it a .toml file and the tricks start to happen; we\u0026rsquo;ll see a bit more later on, for now, let\u0026rsquo;s have a look at the docker-compose file.\nAs you can see, there are not so many details; we\u0026rsquo;re basically specifying two services, a network and setting up some labels to help Traefik does its job. The volumes mappings are mostly optional but, here we\u0026rsquo;re placing some existing configuration and persisting data outside of the containers, at the host file system, no strict rules here as well, in this case for example, since I\u0026rsquo;ve told Ghost that the content path is /var/lib/ghost/content a volume was mounted at this same location so the whole data will be persisted at the host FS, the volumes\u0026rsquo; snapshots will have the same content, so when new EC2 instances are launched, data from previous instances will be present.\nThe labels as mentioned before are to help Traefik but for now just notice the mapping to docker.sock, if you\u0026rsquo;re not familiar with its usage, is basically the Unix socket that the Docker daemon listens to so containers can communicate with it from within, basically containers are able to consume the API through the socket. Traefik, in this case, observe the Docker events through the API and depending on the events, decides what to do about the current configuration, if something needs to be destroyed, created, recreated, changed and so on.\nTraefik # Traefik is a very clever reverse proxy; it deals by itself with many complexities for a low price in terms of configuration as you can see below.\nFrom this short configuration we have support to HTTP/HTTPS, ports configuration, the whole boilerplate to generate our certificates with Let\u0026rsquo;s Encrypt and a dashboard.\nConcerning the dashboard, worth to mention that using the [api] section exposes Traefik\u0026rsquo;s configuration so remember to secure it with some authentication/authorization mechanism, in this example basic auth was used, when hitting the dashboard URL a pop up asking for credentials will be prompted. The password is generated by htpasswd for example echo $(htpasswd -nb \u0026lt;USER\u0026gt; password) | sed -e s/\\\\$/\\\\$\\\\$/g\nTerraform # Now let\u0026rsquo;s have a look at the Terraform infrastructure definition for both EC2 and Cloudflare, I think a few things should be mentioned.\nFirst, there many ways to define and load variables with Terraform but the ones at this example are being loaded from the environment, when Terraform is executed it\u0026rsquo;ll start the lookup, the environment variables prefixed with TF_VAR will be considered by Terraform. As an example, at the .tf file we have the variable aws_access_key_id, the defined environment variable was TF_VAR_aws_access_key_id, for Terraform it\u0026rsquo;s a match. Second, we can break down the configurations as much as we want, just keep in mind that the files are loaded in alphabetical order, in this case it worked fine since Terraform will start from __a__ws.tf then proceed with __c__loudflare.tf. A necessity here since to define the DNS entries the EC2 instance IP address should be defined already. Third, the remote-exec is not the most optimal approach, would be possible, for example, to have AMIs ready to go with everything necessary installed and configured beforehand. EC2 # As said before, you can have a look at the full files at the repository, but here, I\u0026rsquo;ll break down in sections to give a short explanation for each one.\nVariables definitions, to make our configuration flexible/reusable also to hide sensitive information. Virtual Private Cloud (VPC) since a VPC is not like a regular datacenter with networks, switches, routers and so on but, instead, software-defined, we need(optional in fact) to define a private(isolated) section of the Cloud to launch our instances. VPC Subnet once defined our private Cloud; we need to specify our subnet to provide the behaviours we want, in this case, the subnet is associated with an Internet Gateway, turning this particular subnet into a public one so we can access this particular network space from the external world. Internet Gateway as mentioned above, the intention here is to make a particular subnet accessible from the internet, the gateway will work with the subnet to make it happen. Route Table definition into the VPC to route traffic to the Internet Gateway. Route Table Association to \u0026ldquo;link\u0026rdquo; a Subnet with a Route Table. Security Group to be used inside the VPC (this example shouldn\u0026rsquo;t be used in production, it\u0026rsquo;s exceptionally permissive). Amazon Machine Image (AMI) to specify details such as a particular image to be used to start new instances or to attaching a previously taken volume snapshot for example. Key Pair to provide access to the instance over SSH. EC2 Instance a description of how a new instance should look like, where and how it should be placed, for example, into which subnet, the Security Group to be used, Key Pair, which Volume Snapshot to be also used to execute commands over SSH. Many thinks are possible here; the previous description is about the context of this example. With this in mind, let\u0026rsquo;s have a look at each of these sections.\nVariables\nAs mentioned before, Terraform allows us to define variables so we can make our configurations more flexible also keep sensitive information out of sight, one interesting detail is that we can define default values, for example, we could have something like\nVPC\nThe intention of defining a VPC is to have complete control but is not, in fact, a necessity, in case we skip this definition, a default one will be provided by AWS, but of course, AWS won\u0026rsquo;t guess what we intend.\nVPC Subnet\nThe subnet definition is also important, in this case, there\u0026rsquo;s nothing fancy going on, but we could use it to create different subnets some to be exposed and some to be completely private.\nInternet Gateway\nTo achieve the goal of routing internet traffic into the VPC, we need to specify an Internet Gateway.\nRoute Table\nThe route table is what we use in association with the gateway to exposing a particular subnet to the internet.\nRoute Table Association\nThis small section is the glue between a subnet and a route table; we use it to put them together.\nSecurity Group\nBy default, AWS allows all traffic to go outside but terraform doesn\u0026rsquo;t so we need to state this definition explicitly, in this case, in and out.\nAMI\nThis section is important to define how the instances should \u0026ldquo;look like\u0026rdquo;, in this case, as you can see, we have three sections.\ndata: aws_ebs_snapshot to lookup for a specific volume snapshot to be mounted when creating new instances. data: aws_ami to lookup for a specific AMI to be used as a base for the new instance. resource: aws_ami to state the definitions of how the instances should be created, you can see that we\u0026rsquo;ve made use of both filterings we\u0026rsquo;ve defined above, one to set the block device and another to set which particular AMI to be used when instantiating. The snapshots are taken automatically, the periodicity and target were defined beforehand through the Elastic Block Store Lifecycle Manager.\nKey Pair\nHere is the definition of which Key Pair to be associated with the new instances, this way if in possession of the .pem file, access to the instance over SSH is possible.\nEC2 Instance\nOk, here is where we put everything together that\u0026rsquo;s why this section is slightly bigger, but if you read it line by line, you\u0026rsquo;ll notice that now we\u0026rsquo;re only setting values using what was defined before so you can see that there are only three sections.\nValues definitions the first seven lines where the values are coming from what was configured before. Connection configuration lines from nine to fourteen are describing how the connectivity should work. Commands the remaining lines (except for the tag definition) are the commands to be executed while spinning up a new instance. The interesting thing here is about remote-exec because Docker is being installed and its service needs to be started; a new connection should happen, permissions also won\u0026rsquo;t work correctly if the connection is not closed, only from a new session the remaining commands will work appropriately.\nAnother detail to is the usage of tags, once the infrastructure is created, these tags will be present on each of these parts, they\u0026rsquo;re handy for organization and filtering purposes, at the end of the day I was feeling like tagging here was so important and useful as if I was doing something on Kubernetes for example.\nCloudflare # A record defining the target domain, associated with the new instance IP address (we could be pointing to an LB for example). Canonical Name record (CNAME) to provide www. subdomain. Canonical Name record (CNAME) to provide access to Traefik dashboard, do you remember the labels at the docker-compose.yml? We\u0026rsquo;ve defined two traefik.frontend.rule entries, one for Ghost and another to Traefik so, given the requested URL, Traefik will redirect the request to the correct container. The Cloudflare configuration is not extensive so let\u0026rsquo;s keep it as a single piece, there\u0026rsquo;s nothing mythical here but one line that worth to mention is the line 12. Here the IP associated with the new instance is taken and set to the new DNS entry, at first I got some errors at this part because the IP wasn\u0026rsquo;t yet defined, the line cloud-init status --wait at the end of the aws_instance creation tackles this problem, it will make Terraform wait until AWS finishes its job before carrying on with Cloudflare.\nConclusion # After playing with this, apart of the usual excitement, it feels like we need something more powerful to work with configurations, I can imagine the number of files and probably untrackable repetitions for more complex configurations, fingers crossed that we have something better any time soon, maybe Dhall will be better and yet safe option, who knows.\nSo even with the current configuration scenario is quite fun to go through all this and see a whole infrastructure being created/destroyed in a matter of seconds thanks to the automation capabilities, tools and options available we have these days.\n"},{"id":5,"href":"/posts/0003-BuildingBlocks/","title":"BuildingBlocks","section":"Posts","content":"Today I would like to write a bit about Java Collections. I believe most of us that already played with Java know about the primary and most used classes in the Java Collections framework, at least ones that ever had an interview are familiar with, they are part of the default pack of questions that interviewers ask to check if you at least read about Java.\nBut this post does not intend to talk about the different aspects of data structures, or when to use A or B, the idea is to speak briefly about collections in a concurrent context.\nThere are some aspects of Java that were real reasons for concern in the past and still causing skepticism nowadays, example EJBs, reflection or concurrency in general. Who does not have that strange feeling when someone mention Vector, HashTable or StringBuffer? And who do not transfer those mixed feelings to Concurrent\u0026lt;Something\u0026gt;Set,Map,Queue,Deque... for example?\nSomething got lost in the way about concurrency. Vector, HashTable and StringBuffer are synchronized not concurrent. There are differences.\nRegular collections # I\u0026rsquo;ll assume that we don\u0026rsquo;t need to spend much time over here, you probably have been playing with HashSet, ArrayList, LinkedList and HashMap at some point. These classes are just great and probably another way to verify Pareto\u0026rsquo;s principle in the Java context.\nSo if you know you\u0026rsquo;re not sharing your collection at any moment, use them and be happy. By sharing, I mean something around this.\nSynchronized collections # Now we got to the point that is probably one of the causes behind all the mysticism around race condition control slowness, contention. But what could cause contention to classes such as Vector or HashTable? To answer that let\u0026rsquo;s take a look into Collections.synchronizedList and see what happen when we use synchronized collections.\n// The first step is a simple check to verify if the provided List // is an instance of RandomAccess and decide who is wrapping it. public static List \u003c T \u003e synchronizedList(List \u003c T \u003e list) { return (list instanceof RandomAccess ? new SynchronizedRandomAccessList\u003c\u003e(list) : new SynchronizedList\u003c\u003e(list)); } // Now the SynchronizedList constructor receives our List and keeps // an internal reference to it, just notice that SynchronizedList is // passing this list to it's parent SynchronizedCollection, another // static inner class inside Collections SynchronizedList(List \u003c E \u003e list) { super(list); this.list = list; } // This is how SynchronizedCollection's constructor looks like, once // again it keeps an internal reference, but the important part is the // mutex that will be used to synchronize almost all methods around the // original List. SynchronizedCollection(Collection \u003c E \u003e c) { this.c = Objects.requireNonNull(c); mutex = this; } // What's the mutex for public boolean add (E e){ synchronized (mutex) { return c.add(e); } } public E get (int index){ synchronized (mutex) { return list.get(index); } } public boolean removeAll (Collection \u003c ? \u003e coll){ synchronized (mutex) { return c.removeAll(coll); } } As you can see, except for spliterators and streams related methods, all the other ones are synchronized this way, blocking entire methods from outside using the same object to lock everything, this is thread safe but when a thread is holding this lock, doesn\u0026rsquo;t matter what\u0026rsquo;s going on, no one else will perform anything over this Collection. And the mechanism is similar for the other synchronized collections too. If you use one of the methods below, this is how you\u0026rsquo;re protecting your collection against race conditions.\nCollections.synchronizedList(List list) Collections.synchronizedCollection(Collection c) Collections.synchronizedMap(Map m) Collections.synchronizedNavigableMap(NavigableMap m) Collections.synchronizedNavigableSet(NavigableSet s) Collections.synchronizedSet(Set s) Collections.synchronizedSortedMap(SortedMap m) Collections.synchronizedSortedSet(SortedSet s) But that\u0026rsquo;s all right. It doesn\u0026rsquo;t mean you\u0026rsquo;ll have an awful performance just because you\u0026rsquo;re using it. Then when should we use a synchronized collection?\nIf the collection is shared, but access is not too frequent, we\u0026rsquo;re safe to use it. Just remember if you\u0026rsquo;re using large collections it won\u0026rsquo;t scale that well and even using a synchronized collection still necessary to provide additional locking to guard compound actions, e.g.:\nIteration Navigation Conditional operations - putIfAbsent Concurrent collections # Concurrent collections are specific versions designed with concurrency in mind by design, instead of using a single collection-wide lock it uses the concept of segmentation supported by (internal data structures).\nLet\u0026rsquo;s use ConcurrentHashMap as an example, segments (see upfront) will be locked just during updating operations and even during these operations, synchronization will happen in specific moments, is almost surgical. Take a look for example into ConcurrentHashMap.putVal.\nOne of ConcurrentHashMap's inner classes is Segment kept in a bucket table that holds chunks of this HashMap, this way different threads can operate in different segments reducing contention.\nSegment\u0026#x3C;K,V\u0026#x3E;[] segments = (Segment\u0026#x3C;K,V\u0026#x3E;[]) new Segment\u0026#x3C;?,?\u0026#x3E;[DEFAULT_CONCURRENCY_LEVEL]; DEFAULT_CONCURRENCY_LEVEL is the \u0026ldquo;expected\u0026rdquo; number of concurrently updating threads operating over this Map, by default setted to 16 meaning that during high concurrency moments at least 16 threads can operate at the same time over this Map (We\u0026rsquo;ll have 16 locks, each one guarding each segment or bucket if you prefer).\nIf you take a look at the ConcurrentHashMap documentation you\u0026rsquo;ll see that we can change the default values to something more realistic for the scenario in hand, apart of concurrencyLevel is possible to specify loadFactor that tells how much the map should grow in case it has to, by default the growth factor is 0.75%. The initialCapacity, if we have an idea of how big will be the Map, we can avoid the resizing overhead using this parameter, the default again is 16.\nAt this point, we know that internally the ConcurrentHashMap breaks it\u0026rsquo;s content in segments so threads can work in different parts of it simultaneously without interference.\nAll right then, but how does it know where to go which segment when reading or writing content after breaking it up?\nBased on the key hash calc int hash = spread(key.hashCode()) the bucket is identified, created or resized than the new Node\u0026lt;k,v\u0026gt; is inserted. At this point the insertion, we finally have a synchronized block.\nWith this notion about the internal complexity difference between a synchronized and a concurrent collection, you may are wondering when to use the concurrent option.\nThe answer is if the collection will be shared frequently and accessed by multiple threads, for sure a concurrent collection would be handy. Just remember, it might use more memory, especially ConcurrentHashMap why? To support all this mechanism, each Segment is a ReentrantLock, internally ReentrantLock maintain an inner class called Sync that extends AbstractQueuedSynchronizer that holds a queue of nodes to maintain the state of the threads, ending up in few more memory usage. At this point, you can see things such as.\n/** * The thread that enqueued this node. Initialized on * construction and nulled out after use. */ volatile Thread thread; volatile int waitStatus; So on... Ok, I got it! I won\u0026rsquo;t use it otherwise memory will blow up!!\nNo. Not necessarily, I\u0026rsquo;ve performed some silly tests between HashMap, synchronized HashMap, and ConcurrentHashMap to see the difference.\nThe first round 1_000_000 writes and reads with a single Thread.\n# HashMap Time = 3.8s totalMemoryAllocated=538.500MB # Synchronized HashMap Time = 3.9s totalMemoryAllocated=540.500MB # Concurrent HashMap Time = 5.5s totalMemoryAllocated=541.500MB The second round 1_000_000 writes and reads against four threads.\n# HashMap Time = 4.1s totalMemoryAllocated=538.500MB # Synchronized HashMap Time = 3.6s totalMemoryAllocated=539.500MB # Concurrent HashMap Time = 4.5s totalMemoryAllocated=540.000MB As you can see, not bad at all so unless you\u0026rsquo;re fighting for each nanosecond you better think first about preventing wrong results and/or race condition problems.\nBonus # If you\u0026rsquo;re not happy with the alternatives provided by the Java language, there are options out there, some already well known, e.g., Google Guava or more specific one\u0026rsquo;s example:\nEclipse Collections\nEclipse Collections is the best Java collections framework ever that brings happiness to your Java development.\nMinimize object creation Work with large data sets Support mutable and immutable collections JCTools\nSupports various specialized queues, such as\nSPSC - Single Producer Single Consumer (Wait-Free, bounded and unbounded) MPSC - Multi-Producer Single Consumer (Lockless, bounded and unbounded) SPMC - Single Producer Multi-Consumer (Lockless, bounded) MPMC - Multi-Producer Multi-Consumer (Lockless, bounded) Cliff Click\u0026rsquo;s High Scale Library\nA collection of Concurrent and Highly Scalable Utilities. These are intended as direct replacements for the java.util.* or java.util.concurrent.* collections but with better performance when many CPUs are using the collection concurrently. Single-threaded performance may be slightly lower.\nConclusion # That fear of certain Java mechanisms performance are gone or at least minimized, today they are real alternatives to solve practical problems and knowing them is important to avoid coding mistakes or debates using arguments from 10+ years ago.\nKnowing a bit more about the nuances of the available options is useful not to banish a tool but to choose it correctly at the proper moment.\nReferences # ConcurrentHashMap\nCopyOnWriteArrayList\nCopyOnWriteArraySet\nConcurrentHashMap.KeySetView\nConcurrentLinkedDeque.html\nConcurrentLinkedQueue.html\nConcurrentMap.html\nConcurrentNavigableMap.html\nConcurrentSkipListMap.html\nConcurrentSkipListSet.html\n"},{"id":6,"href":"/posts/0002-ThreadSafety/","title":"ThreadSafety","section":"Posts","content":"A few days ago I ended up into this StackOverflow thread from almost seven years ago, this guy was getting inconsistent results from a multithread code. Well, looked like an excellent excuse to recall some topics and do some new research.\nTo start with, let\u0026rsquo;s just quickly remember the differences between Multiprogramming, Multiprocessing, Multitasking, and Multithreading.\nMultiprogramming # In a multiprogramming environment more than one program can be loaded to the main memory, but only one will use the CPU to execute instructions, inside this sort of environment programs that are not using CPU are waiting their turn, but off course this is not acceptable, imagine if the same program hold the CPU for 1h? To solve this problem, the OS will interrupt the current program as soon it starts to perform non CPU required operations, giving the control to one of the programs in memory ready to execute this process is a.k.a context switch. This way no CPU time is wasted waiting, e.g., I/O tasks to finish or to hope that the program will voluntarily release the CPU.\nNotice that at this point we\u0026rsquo;re talking about whole programs\nMultiprocessing # To the end user multiprocessing may look similar to multiprogramming since we see multiple programs executing at the same time, but different of multiprogramming that perform the trick through context switch and memory segregation only, multiprocessing is more about CPU units, if the hardware provides more than one processor, more than one program will execute at the same time. Keep in mind that multiprocessing and multiprogramming are not exclusive. A system can use both mechanisms.\nMultitasking # Multitasking is similar to what we have on multiprogramming, yet at this point we\u0026rsquo;re not just talking about programs, we are now talking about programs, processes, tasks, and threads running at the same time if we have more than one CPU or the illusion of \u0026ldquo;same time\u0026rdquo; through context switch if not, although once again, they are not exclusive. The difference now is that we may have small processes or even sub-tasks using the CPU in a fair way, using CPU time quantums defined by the scheduler instead allowing prosses to kidnap the CPU until it blocks.\nMultithreading # Multithreading is a bit different from the previous models. The idea is that now we have multiple sub-segments \u0026ldquo;threads\u0026rdquo; running concurrently inside the context of another process, in other words, threads are child process sharing parent\u0026rsquo;s resources executing independently. This concept is interesting because in the previous model each process had its resource quota to perform a task, in a multithreading environment, the primary process children share parent\u0026rsquo;s resources so, a process can execute multiple computations in parallel through its children with the resources already available. But this advantage comes with some burden too. The programmer now needs to handle race conditions since now children are fighting for their parent\u0026rsquo;s resources. Well, well, well.\nSo what can we do to manage race conditions and prevent the system from ending up inconsistently? There are some approaches.\nStop sharing state across threads Make state immutable Use synchronization whenever the shared mutable data happen To start with, I would like to do another recap, now about the Java thread memory model: As you can see in the image above, Objects are kept in the Heap space and shared across all threads, do you remember the parent-child process idea? Java is the parent and threads the children, so Java threads have access to the parent resources, including the Heap space.\nJVMs can use a technique called escape analysis, by which they can tell that specific objects remain confined to a single thread for their entire lifetime, and that lifetime is bounded by the lifetime of a given stack frame. Such objects can be safely allocated on the stack instead of the heap.\nBut Java threads have its private area, the thread stack, where local variables and partial method results live their lives.\nAnother important aspect when talking about threads is the CPU cache. In a multithreaded environment for performance reasons each thread may keep a copy of the variables inside the CPU cache, in this case, if the thread operating over the CPU A change a variable, there\u0026rsquo;s no guarantee that this change will be replicated back to the main memory so, another thread would be looking at an inconsistent value visibility problem.\nThe mechanism available in Java to address visibility problems is the volatile keyword, declaring a variable volatile indicates that changes to that variable will be written back to the main memory immediately and read as well will happen directly from the main memory. But even volatile could not be enough, let\u0026rsquo;s suppose you need to read first to figure out the next value before writing something, you could end up with race conditions problems inside this small time gap.\nEnough recap let\u0026rsquo;s take a look in four (Thread confinement, Stack confinement, Object confinement and Immutability) different approaches to design a thread safe class. The one giving birth to that code is responsible for addressing race conditions internally, do you remember encapsulation? You apply in this case too. If you encapsulate your class correctly, you can change your race condition control if you have to without penalizing the ones using it.\nThere are many different scenarios we could use to exemplify race conditions situations, but let\u0026rsquo;s see one by dr kabutz. A basic util class used to format dates.\npublic class DateFormatter { private final DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\"); public String format(Date date) { return df.format(date); } public Date parse(String date) throws ParseException { return df.parse(date); } } Thread confinement # If we execute this code 1_000_000 against 4 threads, what do we have?\njava.util.concurrent.ExecutionException: java.lang.NumberFormatException: multiple points\nIt looks like DateFormat is not thread-safe. We could use ThreadLocal to confine our SimpleDateFormat into the current running thread.\n@ThreadSafe public class DateFormatter { private static final ThreadLocal threadLocal = ThreadLocal.withInitial( () -\u003e new SimpleDateFormat(\"yyyy-MM-dd\") ); public String format(Date date) { return threadLocal.get().format(date); } public Date parse(String date) throws ParseException { return threadLocal.get().parse(date); } } Now we are safe, and the GC log looks like below.\ntime = 2038 durationOfLog=PT2.161S numberOfGCs=12 numberOfYoungGCs=12 numberOfOldGCs=0 memoryReclaimedDuringYoung=3.562GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.484MB totalMemoryAllocated=3.563GB averageCreationRate=1.65GB/s timeInGCs=PT0.015351S timeInYoungGCs=PT0.015351S averageTimeInYoungGCs=DoubleSummaryStatistics{count=12, sum=0.015351, min=0.000543, average=0.001279, max=0.002282} timeInOldGCs=PT0S percentageOfTimeInGC=0.71% All right, I may be mistaken but internally ThreadLocal keeps a static class called ThreadLocalMap which keeps an Entry[] called table where Entry extends WeakReference. Again, I\u0026rsquo;m not sure of what I\u0026rsquo;m about to say, but I remember of posts where people were pointing this feature as source of problems like if ThreadLocal maps are not cleaned up properly after a transaction, the next TransactionProcessingTask might inherit values from another previous, unrelated task and there are even articles about of how to clean ThreadLocals so be careful.\nStack confinement # Another solution would be stack confinement, do you remember the first image above? If not please take another look. What about this:\nThe thread stack, it holds local variables and partial results, and plays a part in method invocation and return\n@ThreadSafe public class DateFormatter { public String format(Date date) { final DateFormat df = getDateFormat(); return df.format(date); } public Date parse(String date) throws ParseException { final DateFormat df = getDateFormat(); return df.parse(date); } private DateFormat getDateFormat() { return new SimpleDateFormat(\"yyyy-MM-dd\"); } } Now we have local variables defined when method invocations happen, meaning that our DateFormat will live inside the thread stack and is not leaking to the heap.\nAnd this is how the GC log looks now.\ntime = 4599 durationOfLog=PT4.962S numberOfGCs=63 numberOfYoungGCs=63 numberOfOldGCs=0 memoryReclaimedDuringYoung=15.573GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.508MB totalMemoryAllocated=15.574GB averageCreationRate=3.14GB/s timeInGCs=PT0.0453345S timeInYoungGCs=PT0.0453345S averageTimeInYoungGCs=DoubleSummaryStatistics{count=63, sum=0.045335, min=0.000501, average=0.000720, max=0.002288} timeInOldGCs=PT0S percentageOfTimeInGC=0.91% Again, 1_000_000 rounds against 4 lovable threads.\nEasier but twice slower and the total memory allocated was quite higher too. But still, safe and easy.\nObject confinement # Another option is the object confinement, here we\u0026rsquo;re declaring DateFormat as a private final field and synchronizing the access to it avoiding two+ threads to have access to it at the same time.\n@ThreadSafe public class DateFormatter { @GuardedBy(\"this\") private final DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\"); public synchronized String format(Date date) { return df.format(date); } public synchronized Date parse(String date) throws ParseException { return df.parse(date); } } Let\u0026rsquo;s see the GC log result:\ntime = 7307 durationOfLog=PT7.61S numberOfGCs=98 numberOfYoungGCs=98 numberOfOldGCs=0 memoryReclaimedDuringYoung=4.124GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.492MB totalMemoryAllocated=4.125GB averageCreationRate=555.06MB/s timeInGCs=PT0.0619821S timeInYoungGCs=PT0.0619821S averageTimeInYoungGCs=DoubleSummaryStatistics{count=98, sum=0.061982, min=0.000455, average=0.000632, max=0.002529} timeInOldGCs=PT0S percentageOfTimeInGC=0.81% All right, this approach spare some memory allocation, but the execution was even worse than stack confinement. It may be because we\u0026rsquo;re introducing some contention synchronizing these methods.\nImmutability # This one will look a bit unfair because address the specific problem related to the Date subject but you can abstract it and think about immutability applied to any other scenario.\nA new date API landed with Java 8, a useful class here would be DateTimeFormatter that by definition is thread-safe.\nA formatter created from a pattern can be used as many times as necessary, it is immutable and is thread-safe.\n@ThreadSafe public class DateFormatter { private static final DateTimeFormatter df = DateTimeFormatter.ISO_LOCAL_DATE; public String format(LocalDate date) { return df.format(date); } public LocalDate parse(String date) { return LocalDate.parse(date, df); } } The GC log result once again, 1_000_000 executions and 4 threads.\ntime = 1371 durationOfLog=PT1.434S numberOfGCs=10 numberOfYoungGCs=10 numberOfOldGCs=0 memoryReclaimedDuringYoung=1.999GB memoryReclaimedDuringOld=0.000B maxHeapAfterGC=1.391MB totalMemoryAllocated=2.000GB averageCreationRate=1.39GB/s timeInGCs=PT0.0147572S timeInYoungGCs=PT0.0147572S averageTimeInYoungGCs=DoubleSummaryStatistics{count=10, sum=0.014757, min=0.000533, average=0.001476, max=0.002788} timeInOldGCs=PT0S percentageOfTimeInGC=1.03% What do we have here, 1.3 second and just 2G of memory allocated, not bad at all, apparently Scala people have a point. Thread-safe, quick and cheap.\nConclusion # The conclusion here is inconclusive :) The approach of choice will probably depend on what is more important to the scenario in hand, are you short in memory, do you care more about performance or readability? Don\u0026rsquo;t you care about any of these aspects and what matters to you is to deliver quick and let the problem blow in someone else\u0026rsquo;s hands? you know that there\u0026rsquo;s a special place in hell for you, right?\nDepending on what you\u0026rsquo;re planning to do about your afterlife or about the scenario you have in hands in this world, you should consider carefully the approach you\u0026rsquo;re going to use to address thread-safe, you can make it concurrent proof but you may introduce other problems if you don\u0026rsquo;t look to the options you have first.\nResults summary:\nThread confinement time = 2038 totalMemoryAllocated=3.563GB Stack confinement time = 4599 totalMemoryAllocated=15.574GB Object confinement time = 7307 totalMemoryAllocated=4.125GB Immutability time = 1371 totalMemoryAllocated=2.000GB References # https://www.javaspecialists.eu/archive/Issue229.html https://gabrieletolomei.wordpress.com/miscellanea/operating-systems/virtual-memory-paging-and-swapping/ https://www.geeksforgeeks.org/operating-system-difference-multitasking-multithreading-multiprocessing/ https://medium.com/@unmeshvjoshi/how-java-thread-maps-to-os-thread-e280a9fb2e06 https://dzone.com/articles/java-memory-and-cpu-monitoring-tools-and-technique http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-2.html#jvms-2.5.2 http://tutorials.jenkov.com/java-concurrency/volatile.html\n"},{"id":7,"href":"/posts/0001-Websockets/","title":"Websockets","section":"Posts","content":"Today post is about a small experiment using Spring 5 to play with WebFux, I\u0026rsquo;ve used it to create a small WebSocket controller to \u0026ldquo;simulate\u0026rdquo; an e-mail inbox. The idea was to send some dummy text to the backend, persist it on MongoDB and from time to time check for new messages and send it to the clients connected to the WebSocket endpoint.\nThe Cast # Let\u0026rsquo;s see an overview of all actors involved in this small experiment. This text probably will be much longer than the code necessary to have everything working with Spring 5.\nI\u0026rsquo;ll assume that Spring doesn\u0026rsquo;t need any introduction even because is hard to define it nowadays, if we were back to 2002 ish I would say it is an Ioc container blah blah blah but today\u0026hellip;\nMongoDB # MongoDB is a free and open-source cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with schemata.\nMongoDB Reactive Streams Java Driver # MongoDB Reactive Streams Java Driver, providing asynchronous stream processing with non-blocking back pressure for MongoDB.\nProject Reactor # Non-Blocking Reactive Streams Foundation for the JVM both implementing a Reactive Extensions inspired API and efficient event streaming support.\nThe reactive design pattern is an event-based architecture for asynchronous handling of a large volume of concurrent service requests coming from single or multiple service handlers.\nAnd the Spring Reactor project is based on this pattern and has the clear and ambitious goal of building asynchronous, reactive applications on the JVM.\nWebFlux # Spring WebFlux is an asynchronous framework from the bottom up. It can run on Servlet Containers using the Servlet 3.1 non-blocking IO API as well as other async runtime environments such as netty or undertow.\nWebSocket # WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection.\nThe WebSocket protocol enables interaction between a web client (such as a browser) and a web server with lower overheads, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the connection open. In this way, an ongoing two-way conversation can take place between the client and the server.\nThe actual code # pom.xml # Let\u0026rsquo;s start with the pom.xml file, this way you\u0026rsquo;ll have what it takes to play by yourself with your WebSockets. Let\u0026rsquo;s check the most important parts.\nThe parent pom or the BOM file used to predefine the dependencies versions, so you can add your dependencies without having to figure out about version compatibility between the components.\nBOM stands for Bill Of Materials. A BOM is a particular kind of POM that is used to control the versions of a project’s dependencies and provide a central place to define and update those versions.\nBOM provides the flexibility to add a dependency to our module without worrying about the version that we should depend on.\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.4.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; The next part, the dependencies I\u0026rsquo;ve used, observe that except for the most specific dependencies I didn\u0026rsquo;t configure any version, it\u0026rsquo;s all done by the parent pom.\n\u0026lt;dependencies\u0026gt; \u0026lt;!-- Boot --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-websocket\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-mongodb-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Tooling --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Websocket --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.webjars\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;webjars-locator-core\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.projectreactor\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;reactor-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-lang\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-lang\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.easytesting\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fest-assert\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; I won\u0026rsquo;t add the whole pom here. If you want, you can generate your one, indeed more updated from the SPRING INITIALIZR page. Pick any functionality you need for your project, and it will just generate a skeleton for you.\napplication.properties # This one is a nice to have, especially if you\u0026rsquo;re going to use database connections, you can place your configurations here instead of having them hardcoded and spread through your code. In case you need extra configuration, and you don\u0026rsquo;t know for sure what property you need, you can check the Common application properties.\nFor the today\u0026rsquo;s experiment, what I needed was just some port definition, logging, and MongoDB URI connection.\nserver.port=8080 #logging logging.level.org.springframework.data=debug logging.level.=debug #mongodb spring.data.mongodb.uri=mongodb://r640:27017/chat WebSocket configurations # The code below is all you need to have your initial WebSocket configuration, I saw some more complex code essentially doing the same thing, but I suppose they were from previous versions of Spring, with Spring 5, that\u0026rsquo;s all you need to start.\n@Configuration @EnableWebSocketMessageBroker public class WebSocketConfig implements WebSocketMessageBrokerConfigurer { @Override public void configureMessageBroker(MessageBrokerRegistry config) { config.enableSimpleBroker(\"/topic\"); config.setApplicationDestinationPrefixes(\"/app\"); } @Override public void registerStompEndpoints(StompEndpointRegistry registry) { registry.addEndpoint(\"/email\"); registry.addEndpoint(\"/email\").withSockJS(); registry.addEndpoint(\"/emails\"); registry.addEndpoint(\"/emails\").withSockJS(); } } The interesting points here are first the @EnableWebSocketMessageBroker annotation, here we\u0026rsquo;re telling Spring that you want to provide WebSockets capabilities on your application, config.enableSimpleBroker(\u0026quot;/topic\u0026quot;) tells spring that anything starting with /topic is a WebSocket endpoint where clients can subscribe to, config.setApplicationDestinationPrefixes(\u0026quot;/app\u0026quot;) again gives Spring some information, now it tells that clients will send data to your application through any endpoint starting with /app, registry.addEndpoint(\u0026quot;/email\u0026quot;) here we\u0026rsquo;re telling Spring about the endpoints we want to use the STOMP protocol, notice that we\u0026rsquo;re overriding registerStompEndpoints these endpoints will probably match some @MessageMapping on your controller.\nModel # Here we\u0026rsquo;re primarily providing the definitions of how the MongoDB collection will look like, notice @Document and @Id annotations, they are basically saying to Spring that this class is about collection entries called email identified by the id field, if we want the collection to have a different name we could define Document like @Document(collection = \u0026quot;another_name\u0026quot;)\n@Data @Builder @Document public class Email { @Id private String id; private String content; private final Date sentAt = new Date(); } Repository # This part is surprising if there\u0026rsquo;s no need for anything special from the repository, the only thing needed is an interface extending ReactiveMongoRepository, Spring will provide basic operations out of the box.\nTo get just the newest emails since the last verification all we need is to provide an abstract method with a @Query annotation, and anything else is handled by Spring, what a wonderful world we\u0026rsquo;re living these days :)\n@Repository public interface EmailRepository extends ReactiveMongoRepository { @Query(\"{ 'sentAt' : { $gte : ?0 }}\") Flux findLastOnes(Date lastExecution); } The controller # Please ignore the private Date lastExecution = new Date() over there, by default Spring creates singleton beans so we\u0026rsquo;re safe here and please, stop thinking about a service layer if you\u0026rsquo;re doing so, we don\u0026rsquo;t need it here right now.\n@Controller @EnableScheduling public class EmailController { @Autowired private EmailRepository repository; @Autowired private SimpMessagingTemplate template; private Date lastExecution = new Date(); @MessageMapping(\"/email\") public void email(final Email message) { repository.save(message).block(); } @MessageMapping(\"/emails\") public void emails() { repository.findAll().subscribe(new EmailSubscriber\u003c\u003e()); } @Scheduled(fixedRate = 10000) public void updateClients() { repository.findLastOnes(lastExecution).subscribe(new EmailSubscriber\u003c\u003e()); } private class EmailSubscriber extends BaseSubscriber { @Override protected void hookOnComplete() { lastExecution = new Date(); } @Override protected void hookOnError(Throwable throwable) { template.convertAndSend(\"/topic/email/errors\", throwable.getMessage()); } @Override protected void hookOnNext(T value) { final Email email = (Email) value; email.setContent(HtmlUtils.htmlEscape(email.getContent())); template.convertAndSend(\"/topic/email/updates\", email); } } } Now, the interesting parts here, the @MessageMapping annotated methods are the ones that clients will send requests too, just remember that from the client side, they need to hit for example /app/email instead of just /email, do you remember WebSocketConfig, config.setApplicationDestinationPrefixes(\u0026quot;/app\u0026quot;)?\nTo simulate that small delay from the point when someone sends us an email and the moment we receive it, I\u0026rsquo;ve configured an schedule that runs every 10 seconds, take a look at updateClients. Here some interesting things are going on. First, we\u0026rsquo;re asking the repository to give us the new messages since the last verification and, instead of wait for the answer, we\u0026rsquo;ve subscribed, furthermore when something comes back we\u0026rsquo;ll be notified so we can push content to the connected clients when and just when we have something to do so.\nsubsctibe is a method provided by Flux that in the Reactor world represents a Reactive stream of size from 0 to N, for 0 or single result Mono is the right representation. Ok, we have some results, what happens now? The Subscriber hookOnNext method will be executed once for each of the found results. Here we can do some work like escape characters. Because we need at least one method with more than one line right? From this point just sent it to the proper topic and the subscribed clients will be updated.\nTesting # Testing it was simple too since with a simple @SpringBootTest annotation the whole stack was made available, we only have to do some initial setup to have an integration test up and running.\n@RunWith(SpringJUnit4ClassRunner.class) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT) public class EmailControllerTest { private static final String SUBSCRIBE_TOPIC_ENDPOINT = \"/topic/email/updates\"; private static final String SEND_EMAIL_ENDPOINT = \"/app/email\"; @Autowired private EmailRepository repository; @Autowired private ReactiveCrudRepository operations; @Value(\"${local.server.port}\") private int port; private String URL; private CompletableFuture completableFuture; private WebSocketStompClient stompClient; private StompSession stompSession; @Before public void setup() throws InterruptedException, ExecutionException, TimeoutException { completableFuture = new CompletableFuture\u003c\u003e(); URL = String.format(\"ws://localhost:%d/email\", port); stompClient = new WebSocketStompClient(new SockJsClient(createTransportClient())); stompClient.setMessageConverter(new MappingJackson2MessageConverter()); stompSession = stompClient.connect(URL, new StompSessionHandlerAdapter() { }).get(1, TimeUnit.SECONDS); } @Test public void receiveEmails() throws InterruptedException, ExecutionException, TimeoutException { // Adding a dummy email final Email email = Email .builder() .id(UUID.randomUUID().toString()) .content(RandomStringUtils.randomAlphabetic(10)) .build(); // Send the email through webSocket to be persisted stompSession.send(SEND_EMAIL_ENDPOINT, email); // Subscribing to the notification endpoint as a client stompSession.subscribe(SUBSCRIBE_TOPIC_ENDPOINT, new MessageStompFrameHandler()); // Waiting up to 10s for a notification to match the scheduler final Email emailReceived = completableFuture.get(10, TimeUnit.SECONDS); // Validating content assertThat(emailReceived).isNotNull(); assertThat(emailReceived.getId()).isEqualTo(email.getId()); assertThat(emailReceived.getContent()).isEqualTo(email.getContent()); assertThat(emailReceived.getSentAt()).isEqualTo(email.getSentAt()); // Removing dummy operations.delete(email).block(); // Verifying if is really gone final Email foundEmail = repository.findById(email.getId()).block(); assertThat(foundEmail).isNull(); } private List createTransportClient() { final List transports = new ArrayList\u003c\u003e(1); transports.add(new WebSocketTransport(new StandardWebSocketClient())); return transports; } private class MessageStompFrameHandler implements StompFrameHandler { @Override public Type getPayloadType(StompHeaders stompHeaders) { return Email.class; } @Override public void handleFrame(StompHeaders stompHeaders, Object o) { completableFuture.complete((Email) o); } } } The client # To play here I\u0026rsquo;ve wrote a html, js and css files, placed them inside the src/main/resources/static folder and started the project, when hitting localhost:8080 the client gets connected automatically, and the fun is complete, I\u0026rsquo;m not proud of them, but you can have them index.html, index.js and index.css.\nConclusion # Once again Spring did an excellent job to provide ways to write applications with low effort. With few lines of code, we can see a new paradigm acting over every single layer of our application. And in case you\u0026rsquo;re wondering if it could be applied somewhere else but WebSockets related endpoints, you can use mainly the same dependencies and approach to see similar results happening for example for rest endpoints.\nHope it provided you a glance of what\u0026rsquo;s possible to achieve with this new Spring version and how much fun you can have playing around with these new toys :)\nReferences # Spring Boot\nSpring 5\nWebFux\nMongoDB\nWebSocket\nWebSocket Support on Spring 5\nStomp\nProject Reactor\nReactive Streams\nSpring Initializr\nCommon Spring application properties\n"},{"id":8,"href":"/posts/0000-Building-Docker-images-on-Travis.ci/","title":"Building Docker images on Travis.ci","section":"Posts","content":"Inspired by the posts from Viktor Adam\u0026rsquo;s blog, when I have some spare time I have been playing with Docker and some open source tools to build this blog.\nSo far I have my two little devices a Raspberry Pi and a Rock64 hosting this blog, yes, that\u0026rsquo;s right, you\u0026rsquo;re reading this straight from a Ghost blog behind an NGinx hosted at my home, be nice and say hi!\nAppart of some details like sensible public data management and external CI usage to generate and publish Docker images that is the subject of this post, this is how my stack looks like right now. It will evolve soonish :]\nSensible public data # As we can see from the description of this repo of mine on Docker hub, some environment variables are expected as parameters to create containers. Some sensible data like Cloudflare API key goes to a public repository on Github, the .env file in binary form instead of row text.\nI could keep it somewhere on my local file system and use it to create containers but if I\u0026rsquo;ve learned something from past events is, maintaining files on my machine is the same of losing them sooner or later so, better to have them on the cloud where smarter people than me are taking care of backups.\nHowever, how to keep this kind of data safe in a public space? If you are using git a good option is to use Git Crypt.\nGit Crypt # First, install it, following this guide.\nOnce you have installed it you need a key to lock and unlock the files you want, there are two options, the Symmetric Mode where you export a key and refer it to unlock your files and GPG Mode where you create a key and import the public key wherever you need to unlock files. I have decided to use Symmetric Mode, this time \u0026ldquo;after few disappointments with the GPG Mode\u0026rdquo; . The setup is easy and do not require further steps after you export your key.\nSetting up git-crypt and exporting a symmetric secret key # Now on the git repository initiate, git-crypt like with a regular git init and export your symmetric key. To accomplish this you only need to perform the steps below.\ngit-crypt init git-crypt export-key /lab/security/git-cript-key Now you need to create the .gitattributes at the root of your project and list the files you want to encrypt, the syntax is the same used by .gitignore files. Eg: *.env filter=git-crypt diff=git-crypt\nFrom this point .env will be automatically encrypted before being pushed to the repository. To unlock it in different machines, you need to run git-crypt unlock referring the secret key that you\u0026rsquo;ve just exported, in my case git-crypt unlock /lab/security/git-crypt-key\nIf you have the intention to unlock this files somewhere else, you need to have access to this key there too, the process is merely the same, unlock the files with the same command above.\nSetting up a Trevis.ci account # At this point, you probably have something like that on your GitHub. The next step is to build your images and publish them automatically when you update your repository.\nUsing your GitHub account, Sign Up to Trevis.ci and enable the integration for this repo.\nSetting up the .travis.yml file # To tell Travis what to do, you need a .travis file on your repository, you can learn about it here to create one that works for you. Mine looks like that:\nsudo: required services: - docker script: # setup QEMU - docker run --rm --privileged multiarch/qemu-user-static:register --reset # build images - docker build -t ddns:$DOCKER_TAG -f Dockerfile.$DOCKER_ARCH . # push images - docker login -u=\"$DOCKER_HUB_LOGIN\" -p=\"$DOCKER_HUB_PASSWORD\" - docker tag ddns:$DOCKER_TAG allandequeiroz/cloudflare-ddns:$DOCKER_TAG - docker push allandequeiroz/cloudflare-ddns:$DOCKER_TAG - \u003e if [ \"$LATEST\" == \"true\" ]; then docker tag ddns:$DOCKER_TAG allandequeiroz/cloudflare-ddns:latest docker push allandequeiroz/cloudflare-ddns:latest fi env: matrix: - DOCKER_TAG=arm DOCKER_ARCH=arm LATEST=true - DOCKER_TAG=amd64 DOCKER_ARCH=amd64 Since I\u0026rsquo;m using Raspberry Pi and Rock64, I need to build images for arm architecture, and since I want to be nice to people, I\u0026rsquo;m generating amd ones even that they are useless for me.\nYou can ask Travis to do that defining the matrix section, this way the script part will be executed once for each line of the matrix, in this case, every time I push something new to GitHub, two new images will be generated and pushed to Docker hub.\nAnother interesting detail here is the ability to define environment variables, as you may notice my Docker hub credentials to push images: docker login -u=\u0026quot;$DOCKER_HUB_LOGIN\u0026quot; -p=\u0026quot;$DOCKER_HUB_PASSWORD\u0026quot;\nTravis provides a settings interface where you can define environment variables and use them instead of keeping it public on your repository.\nOk. Now you need to push something new to your repository or Trigger the build from More options on Travis interface, by the end you\u0026rsquo;ll see something like that on Travis, and your images will be available on Docker hub.\nTravis badges # After all this effort you deserve some badges, you can get one straight from your Travis account, just ask for the .svg file related to your repository. Travis update it after each build so take care to not break the build :)\nhttps://api.travis-ci.org/allandequeiroz/cloudflare-ddns.svg\n"}]